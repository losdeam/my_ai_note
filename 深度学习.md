
# [笔记目录](目录.md)
# 深度学习部分
经过又一次面试，发现新兴公司对调参，修改模型的能力要求较高，所有暂缓C++等基础知识的拓展而转攻深度学习。以求能够具备足够的能力进行天池，kaggle的竞赛与独立项目的进行。使得简历中能够有足够的实例展示对模型调参、修改的能力。
跟随天池中的教程进行学习([具体网址](https://tianchi.aliyun.com/course?spm=a2c22.12281897.J_3941670930.5.e34a23b7wA2LaV))

## 1.数据清洗
东西有点多，分个新的文档
[<font size="5">数据清洗</font>](数据清洗.md)
<h5 id="data_1"></h5>

## 2. 基础知识 ([原始链接](https://tianchi.aliyun.com/course/311/3558))
### 2.1 卷积
#### 2.1.1 实例
```python
import torch 
from torch import nn

def corr2d(X, K):  # X 是输入，K是卷积核
    h, w = K.shape  # 获取卷积核的大小
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()  # 累加
    return Y

X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) # 模拟一个输入
K = torch.tensor([[0, 1], [2, 3]])                  # 模拟一个卷积核
corr2d(X, K)
```
tensor([[19., 25.],
        [37., 43.]])
下图就是上面程序的模拟图
![](http://tianchi-media.oss-cn-beijing.aliyuncs.com/dragonball/DL/other/img/5.1_correlation.svg)
### 2.2 填充（Padding）

1. 使卷积后图像分辨率不变，方便计算特征图尺寸的变化
2. 弥补边界信息“丢失”

填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素）。下图我们在原输入高和宽的两侧分别添加了值为0的元素，使得输入高和宽从3变成了5，并导致输出高和宽由2增加到4。下图阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：0×0+0×1+0×2+0×3=0。

![](http://tianchi-media.oss-cn-beijing.aliyuncs.com/dragonball/DL/other/img/卷积填充.png)
### 2.3 步长（Stride）


卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅或步长（stride）。

下图展示了在高上步幅为3、在宽上步幅为2的卷积运算。可以看到，输出第一列第二个元素时，卷积窗口向下滑动了3行，而在输出第一行第二个元素时卷积窗口向右滑动了2列。当卷积窗口在输入上再向右滑动2列时，由于输入元素无法填满窗口，无结果输出。下图阴影部分为输出元素及其计算所使用的输入和核数组元素：0×0+0×1+1×2+2×3=8、0×0+6×1+0×2+0×3=6。


![](http://tianchi-media.oss-cn-beijing.aliyuncs.com/dragonball/DL/other/img/5.2_conv_stride.svg)
### 2.4 池化

对图像进行下采样，降低图像分辨率。

**池化层的作用：使特征图变小，简化网络计算复杂度；压缩特征，提取主要特征**

常见的池化操作可以分为：最大池化（Max Pool）、平均池化（Avg Pool），示意图如下：

![](http://tianchi-media.oss-cn-beijing.aliyuncs.com/dragonball/DL/other/img/池化.png)
### 2.5 卷积和池化输出尺寸计算

假设输入图片的高和宽一致，卷积核的宽和高一致，那么输入图像的尺寸与输出图像的尺寸有如下关系：

其中，$F_{in}$ 是输入图像、k 是卷积核的大小、p 是图像填充的大小、s 是卷积核的步幅、$F_o$ 是输出、$\lfloor 6.6 \rfloor$ 是向下取整的意思，比如结果是 6.6，那么向下取整就是 6

$$F_{o}=\left\lfloor\frac{F_{\text {in }}-k+2 p}{s}\right\rfloor+1$$

![](http://tianchi-media.oss-cn-beijing.aliyuncs.com/dragonball/DL/other/img/卷积与池化输出尺寸计算.png)

除此之外，卷积神经网络还包括许多优化技术，大家可以参考相关资料。
### 2.6 为什么要用卷积来学习呢？

图像都是用方形矩阵来表达的，学习的本质就是要抽象出特征，以边缘检测为例。它就是识别数字图像中亮度变化明显的点，这些点连接起来往往是物体的边缘。

传统的边缘检测常用的方法包括一阶和二阶导数法，本质上都是利用一个卷积核在原图上进行滑动，只是其中各个位置的系数不同，比如3×3的sobel算子计算x方向的梯度幅度，使用的就是下面的卷积核算子。

![](http://tianchi-media.oss-cn-beijing.aliyuncs.com/dragonball/DL/other/img/sobel算子.png)

如果要用sobel算子完成一次完整的边缘检测，就要同时检测x方向和y方向，然后进行融合。这就是两个通道的卷积，先用两个卷积核进行通道内的信息提取，再进行通道间的信息融合。
这就是卷积提取特征的本质，而所有基于卷积神经网络来学习的图像算法，都是通过不断的卷积来进行特征的抽象，直到实现网络的目标。
### 2.7  卷积神经网络的优势在哪？
前面说了全连接神经网络的原理和结构上的缺陷，而这正好是卷积的优势。
1. 学习原理上的改进。

卷积神经网络不再是有监督学习了，不需要从图像中提取特征，而是直接从原始图像数据进行学习，这样可以最大程度的防止信息在还没有进入网络之前就丢失。

2. 学习方式的改进。

前面说了全连接神经网络一层的结果是与上一层的节点全部连接的，100×100的图像，如果隐藏层也是同样大小（100×100个）的神经元，光是一层网络，就已经有 10^8 个参数。要优化和存储这样的参数量，是无法想象的，所以经典的神经网络，基本上隐藏层在一两层左右。而卷积神经网络某一层的结点，只与上一层的一个图像块相连。

用于产生同一个图像中各个空间位置像素的卷积核是同一个，这就是所谓的权值共享。对于与全连接层同样多的隐藏层，假如每个神经元只和输入10×10的局部patch相连接，且卷积核移动步长为10，则参数为：100×100×10×10，降低了2个数量级。
又能更好的学习，参数又低，卷积神经网络当然是可以成功了。


## 3.numpy部分
numpy是人工智能中一个极为好用的库，大部分的算法都可以通过使用该库来进行优化。
[<font size="5">Numpy</font>](Numpy.md)
<h5 id="Numpy返回坐标">返回坐标</h5>

## 4.torch部分
### 4.0 安装
使用 `pip install pytorch`即可快速完成安装。但是大多是情况下仅会安装CPU版本，而GPU版本的话需要根据自己的电脑配置手动进行安装。
### 4.1 基础知识
<p id = "张量"><p>

#### 4.1.1 张量 ([具体笔记](./data/深度学习/tensor.ipynb))
张量(tensor)是torch中的一种特殊的数据形式，可以在GPU上计算。所以通常会把需要进行大量操作的数据均转化为tensor形式进行操作
##### 4.1.1.1常用函数 
###### 创建
- `torch.tensor(data)`
**功能**：**从data创建tensor**
`data`: 数据，可以是list，numpy
`dtype`: 数据类型，默认与data的一致
`device`: 所在设备，cuda/cpu
`requires_grad`: 是否需要梯度
`pin_memory`: 是否存于锁页内存
>
- `torch.from_numpy(ndarray)`
**功能**：**从numpy创建tensor**
注意事项：从torch.from_numpy创建的tensor于原ndarray共享内存，当修改其中一个数据，另一个也将会被改动。
>
- `torch.zeros(*size)`
功能：**依size创建全0张量**


