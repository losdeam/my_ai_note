{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反向传播\n",
    "即误差反向传播，是训练神经网络的重要方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参照tensor.ipynb所写的一个简单的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建议观看视频[【官方双语】深度学习之反向传播算法 上/下 Part 3 ver 0.9 beta](https://www.bilibili.com/video/BV16x411V7Qg/?spm_id_from=333.788.recommend_more_video.-1&vd_source=3b0e33a626cf5e45835cac5d91093908)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "第0次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第0次反向传播后的梯度为:\n",
      "tensor([-142.1652]) tensor([-20.8274])\n",
      "************************************************************\n",
      "************************************************************\n",
      "第1次反向传播前的梯度为:\n",
      "tensor([0.]) tensor([0.])\n",
      "第1次反向传播后的梯度为:\n",
      "tensor([457.2488]) tensor([62.5591])\n",
      "************************************************************\n",
      "************************************************************\n",
      "第2次反向传播前的梯度为:\n",
      "tensor([0.]) tensor([0.])\n",
      "第2次反向传播后的梯度为:\n",
      "tensor([-1468.1261]) tensor([-205.1958])\n",
      "************************************************************\n",
      "************************************************************\n",
      "第3次反向传播前的梯度为:\n",
      "tensor([0.]) tensor([0.])\n",
      "第3次反向传播后的梯度为:\n",
      "tensor([4716.3101]) tensor([654.9400])\n",
      "************************************************************\n",
      "************************************************************\n",
      "第4次反向传播前的梯度为:\n",
      "tensor([0.]) tensor([0.])\n",
      "第4次反向传播后的梯度为:\n",
      "tensor([-15148.5762]) tensor([-2107.7957])\n",
      "************************************************************\n",
      "************************************************************\n",
      "第5次反向传播前的梯度为:\n",
      "tensor([0.]) tensor([0.])\n",
      "第5次反向传播后的梯度为:\n",
      "tensor([48658.9219]) tensor([6766.4028])\n",
      "************************************************************\n",
      "************************************************************\n",
      "第6次反向传播前的梯度为:\n",
      "tensor([0.]) tensor([0.])\n",
      "第6次反向传播后的梯度为:\n",
      "tensor([-156295.5625]) tensor([-21738.1055])\n",
      "************************************************************\n",
      "************************************************************\n",
      "第7次反向传播前的梯度为:\n",
      "tensor([0.]) tensor([0.])\n",
      "第7次反向传播后的梯度为:\n",
      "tensor([502033.6250]) tensor([69820.5938])\n",
      "************************************************************\n",
      "************************************************************\n",
      "第8次反向传播前的梯度为:\n",
      "tensor([0.]) tensor([0.])\n",
      "第8次反向传播后的梯度为:\n",
      "tensor([-1612569.3750]) tensor([-224272.7656])\n",
      "************************************************************\n",
      "************************************************************\n",
      "第9次反向传播前的梯度为:\n",
      "tensor([0.]) tensor([0.])\n",
      "第9次反向传播后的梯度为:\n",
      "tensor([5179694.5000]) tensor([720377.3750])\n",
      "************************************************************\n",
      "tensor([-394994.4688]) tensor([-54933.7227])\n"
     ]
    }
   ],
   "source": [
    "# 首先我们得有训练样本X，Y， 这里我们随机生成\n",
    "x = torch.rand(20, 1) * 10\n",
    "y = 2 * x + (5 + torch.randn(20, 1))\n",
    "\n",
    "# 构建线性回归函数的参数\n",
    "w = torch.randn((1), requires_grad=True)\n",
    "b = torch.zeros((1), requires_grad=True)   # 这俩都需要求梯度\n",
    "\n",
    "# 设置学习率lr为0.1\n",
    "lr = 0.1\n",
    "\n",
    "for iteration in range(10):\n",
    "    # 前向传播\n",
    "    wx = torch.mul(w, x) #将两张量中元素\n",
    "    y_pred = torch.add(wx, b)\n",
    "\n",
    "    \n",
    "    # 计算loss\n",
    "    loss = (0.5 * (y-y_pred)**2).mean()\n",
    "    print(\"*\"*60)\n",
    "    print(f\"第{iteration}次反向传播前的梯度为:\")\n",
    "    try:\n",
    "        print(w.grad.data,b.grad.data)\n",
    "    except :\n",
    "        print(\"梯度不存在\")\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    print(f\"第{iteration}次反向传播后的梯度为:\")\n",
    "    print(w.grad.data,b.grad.data)\n",
    "    print(\"*\"*60)\n",
    "    # 更新参数\n",
    "    b.data.sub_(lr * b.grad)    # 这种_的加法操作时从自身减，相当于-=\n",
    "    w.data.sub_(lr * w.grad)\n",
    "\n",
    "    # 梯度清零\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "\n",
    "print(w.data, b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，参数在经过反向传播后获得了梯度。\n",
    "但是有个问题，torch是怎么知道该计算谁的梯度呢\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过查询资料可以得知，torch.backward()默认情况下只累积叶子节点张量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is leaf: True\n",
      "y is leaf: True\n",
      "w is leaf: True\n",
      "b is leaf: True\n",
      "wx is leaf: False\n",
      "y_pred is leaf: False\n"
     ]
    }
   ],
   "source": [
    "# 首先我们得有训练样本X，Y， 这里我们随机生成\n",
    "x = torch.rand(20, 1) * 10\n",
    "y = 2 * x + (5 + torch.randn(20, 1))\n",
    "\n",
    "# 构建线性回归函数的参数\n",
    "w = torch.randn((1), requires_grad=True)\n",
    "b = torch.zeros((1), requires_grad=True)   # 这俩都需要求梯度\n",
    "wx = torch.mul(w, x) #将两张量中元素\n",
    "y_pred = torch.add(wx, b)\n",
    "\n",
    "print(\"x is leaf:\",x.is_leaf)\n",
    "print(\"y is leaf:\",y.is_leaf)\n",
    "print(\"w is leaf:\",w.is_leaf)\n",
    "print(\"b is leaf:\",b.is_leaf)\n",
    "print(\"wx is leaf:\",wx.is_leaf)\n",
    "print(\"y_pred is leaf:\",y_pred.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那是不是意味着x,y同样也获得了梯度呢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "第0次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第0次反向传播后的梯度为:\n",
      "tensor([-21.9943]) tensor([-5.3803])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\WINDOWS\\TEMP/ipykernel_10308/1924280254.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"第{iteration}次反向传播后的梯度为:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"*\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# 更新参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "# 首先我们得有训练样本X，Y， 这里我们随机生成\n",
    "x = torch.rand(20, 1) * 10\n",
    "y = 2 * x + (5 + torch.randn(20, 1))\n",
    "\n",
    "# 构建线性回归函数的参数\n",
    "w = torch.randn((1), requires_grad=True)\n",
    "b = torch.zeros((1), requires_grad=True)   # 这俩都需要求梯度\n",
    "\n",
    "# 设置学习率lr为0.1\n",
    "lr = 0.1\n",
    "\n",
    "for iteration in range(10):\n",
    "    # 前向传播\n",
    "    wx = torch.mul(w, x) #将两张量中元素\n",
    "    y_pred = torch.add(wx, b)\n",
    "\n",
    "    \n",
    "    # 计算loss\n",
    "    loss = (0.5 * (y-y_pred)**2).mean()\n",
    "    print(\"*\"*60)\n",
    "    print(f\"第{iteration}次反向传播前的梯度为:\")\n",
    "    try:\n",
    "        print(w.grad.data,b.grad.data,x.grad.data,y.grad.data)\n",
    "    except :\n",
    "        print(\"梯度不存在\")\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    print(f\"第{iteration}次反向传播后的梯度为:\")\n",
    "    print(w.grad.data,b.grad.data)\n",
    "    print(x.grad.data,y.grad.data)\n",
    "    print(\"*\"*60)\n",
    "    # 更新参数\n",
    "    b.data.sub_(lr * b.grad)    # 这种_的加法操作时从自身减，相当于-=\n",
    "    w.data.sub_(lr * w.grad)\n",
    "\n",
    "    # 梯度清零\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "\n",
    "print(w.data, b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然不太行，发现w,b后面有requires_grad，那如果去掉是不是连w,b也没法求梯度了呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "第0次反向传播前的梯度为:\n",
      "梯度不存在\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\WINDOWS\\TEMP/ipykernel_10308/2710336761.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"梯度不存在\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# 反向传播\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"第{iteration}次反向传播后的梯度为:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\d2l\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\d2l\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# 首先我们得有训练样本X，Y， 这里我们随机生成\n",
    "x = torch.rand(20, 1) * 10\n",
    "y = 2 * x + (5 + torch.randn(20, 1))\n",
    "\n",
    "# 构建线性回归函数的参数\n",
    "w = torch.randn((1), requires_grad=False)\n",
    "b = torch.zeros((1), requires_grad=False)   # 这俩都需要求梯度\n",
    "\n",
    "# 设置学习率lr为0.1\n",
    "lr = 0.1\n",
    "\n",
    "for iteration in range(10):\n",
    "    # 前向传播\n",
    "    wx = torch.mul(w, x) #将两张量中元素\n",
    "    y_pred = torch.add(wx, b)\n",
    "\n",
    "    \n",
    "    # 计算loss\n",
    "    loss = (0.5 * (y-y_pred)**2).mean()\n",
    "    print(\"*\"*60)\n",
    "    print(f\"第{iteration}次反向传播前的梯度为:\")\n",
    "    try:\n",
    "        print(w.grad.data,b.grad.data,x.grad.data,y.grad.data)\n",
    "    except :\n",
    "        print(\"梯度不存在\")\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    print(f\"第{iteration}次反向传播后的梯度为:\")\n",
    "    print(w.grad.data,b.grad.data)\n",
    "    print(x.grad.data,y.grad.data)\n",
    "    print(\"*\"*60)\n",
    "    # 更新参数\n",
    "    b.data.sub_(lr * b.grad)    # 这种_的加法操作时从自身减，相当于-=\n",
    "    w.data.sub_(lr * w.grad)\n",
    "\n",
    "    # 梯度清零\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "\n",
    "print(w.data, b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确实，那么如果我们为x,y也加入requires_grad=True ，那是不是意味着x,y同样也能获取梯度呢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "第0次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第0次反向传播后的梯度为:\n",
      "tensor([-3.9470]) tensor([-6.2540])\n",
      "************************************************************\n",
      "************************************************************\n",
      "第1次反向传播前的梯度为:\n",
      "tensor([0.]) tensor([0.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\WINDOWS\\TEMP/ipykernel_10308/1105722964.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"梯度不存在\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# 反向传播\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"第{iteration}次反向传播后的梯度为:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\d2l\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\d2l\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# 首先我们得有训练样本X，Y， 这里我们随机生成\n",
    "x = torch.rand(20, 1, requires_grad=True)\n",
    "y = 2 * x + (5 + torch.randn(20, 1, requires_grad=True))\n",
    "\n",
    "# 构建线性回归函数的参数\n",
    "w = torch.randn((1), requires_grad=True)\n",
    "b = torch.zeros((1), requires_grad=True)   # 这俩都需要求梯度\n",
    "\n",
    "# 设置学习率lr为0.1\n",
    "lr = 0.1\n",
    "\n",
    "for iteration in range(10):\n",
    "    # 前向传播\n",
    "    wx = torch.mul(w, x) #将两张量中元素\n",
    "    y_pred = torch.add(wx, b)\n",
    "\n",
    "    \n",
    "    # 计算loss\n",
    "    loss = (0.5 * (y-y_pred)**2).mean()\n",
    "    print(\"*\"*60)\n",
    "    print(f\"第{iteration}次反向传播前的梯度为:\")\n",
    "    try:\n",
    "        print(w.grad.data,b.grad.data)\n",
    "    except :\n",
    "        print(\"梯度不存在\")\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    print(f\"第{iteration}次反向传播后的梯度为:\")\n",
    "    print(w.grad.data,b.grad.data)\n",
    "    print(\"*\"*60)\n",
    "    # 更新参数\n",
    "    b.data.sub_(lr * b.grad)    # 这种_的加法操作时从自身减，相当于-=\n",
    "    w.data.sub_(lr * w.grad)\n",
    "\n",
    "    # 梯度清零\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "\n",
    "print(w.data, b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出现了些奇怪的错误，报错的意思就是，因为某个带有梯度信息的变量在被执行了一次后，这些梯度信息就被计算图释放掉了，而我们的代码却尝试第二次反向传播的时候来访问这些变量(梯度信息)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在查询资料后发现在backward后添加retain_graph=True就可以在每次反向传播后暂时保留中间节点的梯度值，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "第0次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第0次反向传播后的梯度为:\n",
      "tensor([-3.4236]) tensor([-5.9324]) tensor([[0.3802],\n",
      "        [0.4118],\n",
      "        [0.3724],\n",
      "        [0.5003],\n",
      "        [0.3666],\n",
      "        [0.3428],\n",
      "        [0.4192],\n",
      "        [0.3371],\n",
      "        [0.4389],\n",
      "        [0.4398],\n",
      "        [0.4621],\n",
      "        [0.5241],\n",
      "        [0.4327],\n",
      "        [0.5485],\n",
      "        [0.4095],\n",
      "        [0.4476],\n",
      "        [0.3266],\n",
      "        [0.3738],\n",
      "        [0.4188],\n",
      "        [0.4678]]) None\n",
      "************************************************************\n",
      "************************************************************\n",
      "第1次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第1次反向传播后的梯度为:\n",
      "tensor([-2.9584]) tensor([-5.1490]) tensor([[0.6334],\n",
      "        [0.6851],\n",
      "        [0.6225],\n",
      "        [0.8326],\n",
      "        [0.6112],\n",
      "        [0.5539],\n",
      "        [0.6993],\n",
      "        [0.5525],\n",
      "        [0.7277],\n",
      "        [0.7272],\n",
      "        [0.7623],\n",
      "        [0.8761],\n",
      "        [0.7213],\n",
      "        [0.9152],\n",
      "        [0.6844],\n",
      "        [0.7407],\n",
      "        [0.5300],\n",
      "        [0.6216],\n",
      "        [0.6911],\n",
      "        [0.7787]]) None\n",
      "************************************************************\n",
      "************************************************************\n",
      "第2次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第2次反向传播后的梯度为:\n",
      "tensor([-2.5551]) tensor([-4.4697]) tensor([[0.7947],\n",
      "        [0.8587],\n",
      "        [0.7834],\n",
      "        [1.0439],\n",
      "        [0.7675],\n",
      "        [0.6761],\n",
      "        [0.8785],\n",
      "        [0.6834],\n",
      "        [0.9093],\n",
      "        [0.9065],\n",
      "        [0.9484],\n",
      "        [1.1026],\n",
      "        [0.9057],\n",
      "        [1.1501],\n",
      "        [0.8611],\n",
      "        [0.9240],\n",
      "        [0.6495],\n",
      "        [0.7786],\n",
      "        [0.8600],\n",
      "        [0.9766]]) None\n",
      "************************************************************\n",
      "************************************************************\n",
      "第3次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第3次反向传播后的梯度为:\n",
      "tensor([-2.2056]) tensor([-3.8807]) tensor([[0.8903],\n",
      "        [0.9612],\n",
      "        [0.8797],\n",
      "        [1.1687],\n",
      "        [0.8602],\n",
      "        [0.7404],\n",
      "        [0.9852],\n",
      "        [0.7567],\n",
      "        [1.0152],\n",
      "        [1.0101],\n",
      "        [1.0552],\n",
      "        [1.2383],\n",
      "        [1.0151],\n",
      "        [1.2901],\n",
      "        [0.9668],\n",
      "        [1.0303],\n",
      "        [0.7136],\n",
      "        [0.8711],\n",
      "        [0.9570],\n",
      "        [1.0936]]) None\n",
      "************************************************************\n",
      "************************************************************\n",
      "第4次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第4次反向传播后的梯度为:\n",
      "tensor([-1.9026]) tensor([-3.3701]) tensor([[0.9393],\n",
      "        [1.0134],\n",
      "        [0.9295],\n",
      "        [1.2324],\n",
      "        [0.9079],\n",
      "        [0.7687],\n",
      "        [1.0400],\n",
      "        [0.7918],\n",
      "        [1.0685],\n",
      "        [1.0617],\n",
      "        [1.1079],\n",
      "        [1.3086],\n",
      "        [1.0713],\n",
      "        [1.3622],\n",
      "        [1.0215],\n",
      "        [1.0834],\n",
      "        [0.7426],\n",
      "        [0.9181],\n",
      "        [1.0049],\n",
      "        [1.1534]]) None\n",
      "************************************************************\n",
      "************************************************************\n",
      "第5次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第5次反向传播后的梯度为:\n",
      "tensor([-1.6399]) tensor([-2.9274]) tensor([[0.9556],\n",
      "        [1.0307],\n",
      "        [0.9464],\n",
      "        [1.2536],\n",
      "        [0.9238],\n",
      "        [0.7764],\n",
      "        [1.0584],\n",
      "        [0.8025],\n",
      "        [1.0859],\n",
      "        [1.0783],\n",
      "        [1.1247],\n",
      "        [1.3324],\n",
      "        [1.0901],\n",
      "        [1.3864],\n",
      "        [1.0400],\n",
      "        [1.1006],\n",
      "        [0.7508],\n",
      "        [0.9337],\n",
      "        [1.0202],\n",
      "        [1.1733]]) None\n",
      "************************************************************\n",
      "************************************************************\n",
      "第6次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第6次反向传播后的梯度为:\n",
      "tensor([-1.4123]) tensor([-2.5435]) tensor([[0.9494],\n",
      "        [1.0242],\n",
      "        [0.9399],\n",
      "        [1.2456],\n",
      "        [0.9177],\n",
      "        [0.7742],\n",
      "        [1.0514],\n",
      "        [0.7988],\n",
      "        [1.0795],\n",
      "        [1.0723],\n",
      "        [1.1187],\n",
      "        [1.3233],\n",
      "        [1.0830],\n",
      "        [1.3772],\n",
      "        [1.0329],\n",
      "        [1.0943],\n",
      "        [0.7483],\n",
      "        [0.9278],\n",
      "        [1.0147],\n",
      "        [1.1658]]) None\n",
      "************************************************************\n",
      "************************************************************\n",
      "第7次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第7次反向传播后的梯度为:\n",
      "tensor([-1.2151]) tensor([-2.2107]) tensor([[0.9282],\n",
      "        [1.0019],\n",
      "        [0.9174],\n",
      "        [1.2182],\n",
      "        [0.8969],\n",
      "        [0.7694],\n",
      "        [1.0272],\n",
      "        [0.7875],\n",
      "        [1.0578],\n",
      "        [1.0522],\n",
      "        [1.0989],\n",
      "        [1.2913],\n",
      "        [1.0583],\n",
      "        [1.3451],\n",
      "        [1.0082],\n",
      "        [1.0733],\n",
      "        [0.7418],\n",
      "        [0.9080],\n",
      "        [0.9967],\n",
      "        [1.1400]]) None\n",
      "************************************************************\n",
      "************************************************************\n",
      "第8次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第8次反向传播后的梯度为:\n",
      "tensor([-1.0441]) tensor([-1.9221]) tensor([[0.8972],\n",
      "        [0.9695],\n",
      "        [0.8840],\n",
      "        [1.1784],\n",
      "        [0.8663],\n",
      "        [0.7666],\n",
      "        [0.9915],\n",
      "        [0.7733],\n",
      "        [1.0270],\n",
      "        [1.0242],\n",
      "        [1.0719],\n",
      "        [1.2439],\n",
      "        [1.0222],\n",
      "        [1.2977],\n",
      "        [0.9716],\n",
      "        [1.0439],\n",
      "        [0.7360],\n",
      "        [0.8792],\n",
      "        [0.9721],\n",
      "        [1.1024]]) None\n",
      "************************************************************\n",
      "************************************************************\n",
      "第9次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第9次反向传播后的梯度为:\n",
      "tensor([-0.8959]) tensor([-1.6719]) tensor([[0.8602],\n",
      "        [0.9312],\n",
      "        [0.8434],\n",
      "        [1.1310],\n",
      "        [0.8297],\n",
      "        [0.7690],\n",
      "        [0.9485],\n",
      "        [0.7593],\n",
      "        [0.9914],\n",
      "        [0.9926],\n",
      "        [1.0421],\n",
      "        [1.1862],\n",
      "        [0.9789],\n",
      "        [1.2406],\n",
      "        [0.9272],\n",
      "        [1.0103],\n",
      "        [0.7337],\n",
      "        [0.8453],\n",
      "        [0.9448],\n",
      "        [1.0576]]) None\n",
      "************************************************************\n",
      "tensor([2.5058]) tensor([3.4077])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\d2l\\lib\\site-packages\\torch\\_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:475.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "# 首先我们得有训练样本X，Y， 这里我们随机生成\n",
    "x = torch.rand(20, 1, requires_grad=True)\n",
    "\n",
    "y = 2 * x + (5 + torch.randn(20, 1, requires_grad=True))\n",
    "\n",
    "# 构建线性回归函数的参数\n",
    "w = torch.randn((1), requires_grad=True)\n",
    "b = torch.zeros((1), requires_grad=True)   # 这俩都需要求梯度\n",
    "\n",
    "# 设置学习率lr为0.1\n",
    "lr = 0.1\n",
    "\n",
    "for iteration in range(10):\n",
    "    # 前向传播\n",
    "    wx = torch.mul(w, x) #将两张量中元素\n",
    "    y_pred = torch.add(wx, b)\n",
    "\n",
    "    \n",
    "    # 计算loss\n",
    "    loss = (0.5 * (y-y_pred)**2).mean()\n",
    "\n",
    "    print(\"*\"*60)\n",
    "    print(f\"第{iteration}次反向传播前的梯度为:\")\n",
    "    try:\n",
    "        print(w.grad.data,b.grad.data,x.grad.data,y.grad.data)\n",
    "    except :\n",
    "        print(\"梯度不存在\")\n",
    "    # 反向传播\n",
    "    loss.backward(retain_graph=True)\n",
    "    print(f\"第{iteration}次反向传播后的梯度为:\")\n",
    "    t = x.grad.data if x.grad != None  else None\n",
    "    t1 = y.grad.data if y.grad != None else None\n",
    "    print(w.grad.data,b.grad.data,t,t1)\n",
    "    print(\"*\"*60)\n",
    "    # 更新参数\n",
    "    b.data.sub_(lr * b.grad)    # 这种_的加法操作时从自身减，相当于-=\n",
    "    w.data.sub_(lr * w.grad)\n",
    "\n",
    "    # 梯度清零\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "\n",
    "print(w.data, b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "未经过任何计算的x有了梯度，而经过计算的y没有，那么会是这个原因吗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "第0次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第0次反向传播后的梯度为:\n",
      "tensor([-111.6310]) tensor([-17.1766]) None None\n",
      "************************************************************\n",
      "************************************************************\n",
      "第1次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第1次反向传播后的梯度为:\n",
      "tensor([370.2808]) tensor([56.9763]) None None\n",
      "************************************************************\n",
      "************************************************************\n",
      "第2次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第2次反向传播后的梯度为:\n",
      "tensor([-1228.2245]) tensor([-188.9895]) None None\n",
      "************************************************************\n",
      "************************************************************\n",
      "第3次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第3次反向传播后的梯度为:\n",
      "tensor([4074.0305]) tensor([626.8811]) None None\n",
      "************************************************************\n",
      "************************************************************\n",
      "第4次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第4次反向传播后的梯度为:\n",
      "tensor([-13513.5918]) tensor([-2079.3682]) None None\n",
      "************************************************************\n",
      "************************************************************\n",
      "第5次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第5次反向传播后的梯度为:\n",
      "tensor([44824.6953]) tensor([6897.2837]) None None\n",
      "************************************************************\n",
      "************************************************************\n",
      "第6次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第6次反向传播后的梯度为:\n",
      "tensor([-148683.8906]) tensor([-22878.3438]) None None\n",
      "************************************************************\n",
      "************************************************************\n",
      "第7次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第7次反向传播后的梯度为:\n",
      "tensor([493185.6875]) tensor([75887.6562]) None None\n",
      "************************************************************\n",
      "************************************************************\n",
      "第8次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第8次反向传播后的梯度为:\n",
      "tensor([-1635901.]) tensor([-251719.9688]) None None\n",
      "************************************************************\n",
      "************************************************************\n",
      "第9次反向传播前的梯度为:\n",
      "梯度不存在\n",
      "第9次反向传播后的梯度为:\n",
      "tensor([5426297.]) tensor([834957.3125]) None None\n",
      "************************************************************\n",
      "tensor([-416931.1875]) tensor([-64154.2266])\n"
     ]
    }
   ],
   "source": [
    "# 首先我们得有训练样本X，Y， 这里我们随机生成\n",
    "x = torch.rand(20, 1, requires_grad=True) +6\n",
    "\n",
    "y = 2 * x + (5 + torch.randn(20, 1, requires_grad=True))\n",
    "\n",
    "# 构建线性回归函数的参数\n",
    "w = torch.randn((1), requires_grad=True)\n",
    "b = torch.zeros((1), requires_grad=True)   # 这俩都需要求梯度\n",
    "\n",
    "# 设置学习率lr为0.1\n",
    "lr = 0.1\n",
    "\n",
    "for iteration in range(10):\n",
    "    # 前向传播\n",
    "    wx = torch.mul(w, x) #将两张量中元素\n",
    "    y_pred = torch.add(wx, b)\n",
    "\n",
    "    \n",
    "    # 计算loss\n",
    "    loss = (0.5 * (y-y_pred)**2).mean()\n",
    "\n",
    "    print(\"*\"*60)\n",
    "    print(f\"第{iteration}次反向传播前的梯度为:\")\n",
    "    try:\n",
    "        print(w.grad.data,b.grad.data,x.grad.data,y.grad.data)\n",
    "    except :\n",
    "        print(\"梯度不存在\")\n",
    "    # 反向传播\n",
    "    loss.backward(retain_graph=True)\n",
    "    print(f\"第{iteration}次反向传播后的梯度为:\")\n",
    "    t = x.grad.data if x.grad != None  else None\n",
    "    t1 = y.grad.data if y.grad != None else None\n",
    "    print(w.grad.data,b.grad.data,t,t1)\n",
    "    print(\"*\"*60)\n",
    "    # 更新参数\n",
    "    b.data.sub_(lr * b.grad)    # 这种_的加法操作时从自身减，相当于-=\n",
    "    w.data.sub_(lr * w.grad)\n",
    "\n",
    "    # 梯度清零\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "\n",
    "print(w.data, b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给x加点东西，发现真的没法计算梯度了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么如何计算参数梯度的前提要求就已经确定了\n",
    "1. 使用torch定义一个未经过运算的张量\n",
    "2. 为张量设定requires_grad=True\n",
    "3. 确保张量为叶节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实操部分结束，那么该回到枯燥的理论部分了\n",
    "<p id = \"end\"><p>\n",
    "\n",
    "[返回深度学习笔记](../../深度学习.md)\n",
    "\n",
    "[直接前往反向传播部分](./torch_basic_backward.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
