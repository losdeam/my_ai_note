{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "81581688-5d84-43b8-9aaa-23f6bb4126f0"
   },
   "source": [
    "[【官方双语】深度学习之神经网络的结构](https://b23.tv/XNhooa)  \n",
    "[【官方双语】深度学习之梯度下降法](https://b23.tv/bAS3Me)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "580a1507-988e-4428-8465-539325250b95"
   },
   "source": [
    "# softmax的基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "d346c990-2896-4e89-88d3-2a685077afae"
   },
   "source": [
    "- 分类问题：\n",
    "    - 一个简单的图像分类问题，输入图像的高和宽均为2像素，色彩为灰度。\n",
    "    - 图像中的4像素分别是$x_1$，$x_2$，$x_3$，$x_4$。\n",
    "    - 假设真实标签为狗、猫或者鸡，这些标签对应的离散值为$y_1$，$y_2$，$y_3$。\n",
    "    - 我们通常使用离散的数值来表示类别，例如$y_1=1$，$y_2=2$，$y_3=3$。\n",
    "- 权重矢量\n",
    "    - $o_1=x_1w_{11}+x_2w_{21}+x_3w_{31}+x_4w_{41}+b_1$\n",
    "    - $o_2=x_1w_{12}+x_2w_{22}+x_3w_{32}+x_4w_{42}+b_2$\n",
    "    - $o_3=x_1w_{13}+x_2w_{23}+x_3w_{33}+x_4w_{43}+b_3$\n",
    "- 神经网络图\n",
    "    - 下图用神经网络图描绘了上面的计算。softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出$o_1$，$o_2$，$o_3$的计算都要依赖于所有的输入$x_1$，$x_2$，$x_3$，$x_4$，softmax回归的输出层也是一个全连接层。 \n",
    "    \n",
    "    <img src=\"https://cdn.nlark.com/yuque/0/2021/jpeg/1508544/1613313946469-ca66b5b1-4365-404d-9777-e7bf9abf890c.jpeg\"/>\n",
    "\n",
    "既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值当作预测类别是的置信度，并将值最大的输出所对应的类作为预测输出，即 输出$\\arg\\max_i o_i$。例如，如果$o_1$，$o_2$，$o_3$分别是0.1,10,0.1,由于$o_2$最大，那么预测类别为2，其代表猫。\n",
    "- 输出问题\n",
    "    - 直接使用输出层的输出有两个问题：\n",
    "    - 一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。例如，刚才举的例子中的输出值10表示“很置信”图像类别为猫，因为输出值是其他两类的输出值的100倍。但如果$o_1=o_3=10^3$，那么输出值10却又表示图像类别为猫的概率很低。\n",
    "    - 另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。\n",
    "- softmax运算符解决了以上两个问题。它通过下式将输出值变换成值为正且和为1的概率分布：\n",
    "    - $\\hat y_1,\\hat y_2,\\hat y_3=softmax(o_1,o_2,o_3)$\n",
    "    - 其中，$\\hat y_1=\\frac{exp(o_1)}{\\sum_{i=1}^3exp(o_i)}$，$\\hat y_2=\\frac{exp(o_2)}{\\sum_{i=1}^3exp(o_i)}$，$\\hat y_3=\\frac{exp(o_3)}{\\sum_{i=1}^3exp(o_i)}$\n",
    "    - 容易看出$\\hat y_1+\\hat y_2+\\hat y_3=1$且$0\\leq \\hat y_1, \\hat y_2, \\hat y_3 \\leq 1$，因此$\\hat y_1$, $\\hat y_2$, $\\hat y_3$是一个合法的概率分布。这时候，如果$\\hat y_2 = 0.8$，不管$\\hat y_1$和$\\hat y_3$的值是多少，我们都知道图像类别为猫的概率为80%。此外，我们注意到$\\arg\\max_i o_i=\\arg\\max_i \\hat y_i$。因此softmax运算不改变预测类别输出。\n",
    "- 计算效率\n",
    "    - 单样本矢量计算表达式\n",
    "    - 为了提高计算效率，我们可以将单样本分类通过矢量计算来表达。在上面的图像分类问题中，假设softmax回归的权重和偏差参数分别为\n",
    "        - $W=\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13}\\\\\n",
    "w_{21} & w_{22} & w_{23}\\\\\n",
    "w_{31} & w_{32} & w_{33}\\\\\n",
    "w_{41} & w_{42} & w_{43}\n",
    "\\end{bmatrix}$，$b=[b_1, b_2, b_3]$\n",
    "        - 设高和宽分别为2个像素的图像样本的特征为：$x^{(i)}=[x_1^{(i)} \\ x_2^{(i)} \\ x_3^{(i)} \\ x_4^{(i)}]$\n",
    "        - 输出层的输出为：$o^{(i)}=[o_1^{(i)} \\ o_2^{(i)} \\ o_3^{(i)}]$\n",
    "        - 预测为狗、猫或鸡的概率分布为：$\\hat y^{(i)}=[\\hat y_1^{(i)} \\ \\hat y_2^{(i)} \\ \\hat y_3^{(i)}]$\n",
    "        - softmax回归对样本分类的矢量计算表达式为：$\\begin{align}\n",
    "o^{(i)}=x^{(i)}W+b\\\\\n",
    "\\hat y^{(i)}=softmax(o^{(i)})\n",
    "\\end{align}$\n",
    "    - 小批量矢量计算表达式\n",
    "    - 为了进一步提升计算效率，我们通常对小批量数据做矢量计算。广义上讲，给定一个小批量样本，其批量大小为$n$，输入个数（特征数）为$d$，输出个数（类别数）为$q$。设批量特征为$X\\in R^{n×d}$。假设softmax回归的权重和偏差参数分别为$W\\in R^{d×q}$和$b\\in R^{1×q}$。softmax回归的矢量计算表达式为：$\\begin{align}\n",
    "O=XW+b\\\\\n",
    "\\hat Y=softmax(O)\n",
    "\\end{align}$\n",
    "- 其中的加法运算使用了广播机制，$O,\\hat Y\\in R^{n×q}$且这两个矩阵的第$i$行分别为样本$i$的输出$o^{(i)}$和概率分布$\\hat y^{(i)}$。\n",
    "\n",
    "# 交叉熵损失函数\n",
    "- 对于样本，我们构造向量$y^{(i)}\\in R^q$，使其第$y^{(i)}$（样本$i$类别的离散数值）个元素为1，其余为0。这样我们的训练目标可以设为使预测概率分布$\\hat y^{(i)}$尽可能接近真实的标签概率分布$y^{(i)}$。\n",
    "- 平方损失估计：$Loss=|\\hat y^{(i)}-y^{(i)}|^2/2$\n",
    "- 然而，想要预测分类结果正确，我们其实并不需要预测概率完全等于标签概率。例如，在图像分类的例子里，如果$y^{(i)}=3$，那么我们只需要$\\hat y_3^{(i)}$比其他两个预测值$\\hat y_1^{(i)}$和$\\hat y_2^{(i)}$大就行了。即使$\\hat y_3^{(i)}$值为0.6，不管其他两个预测值为多少，类别预测均正确。而平方损失则过于严格，例如$\\hat y_1^{(i)}=\\hat y_2^{(i)}=0.2$比$\\hat y_1^{(i)}=0, \\hat y_2^{(i)}=0.4$的损失要小很多，虽然两者都有同样正确的分类预测结果。\n",
    "- 改善上述问题的一个方法是使用更合适衡量两个概率分布差异的测量函数。其中，交叉熵是一个常用的衡量方法：$H\\left(y^{(i)}, \\hat y^{(i)}\\right)=-\\sum_{j=1}^qy_j^{(i)}\\log \\hat y_j^{(i)}$\n",
    "- 假设训练数据集的样本数为，交叉熵损失函数定义为：$L(\\theta)=\\frac{1}{n}\\sum_{i=1}^nH\\left(y^{(i)}, \n",
    "\\hat y^{(i)}\\right)$\n",
    "- 其中$\\theta$代表模型参数。同样地，如果每个样本只有一个标签，那么交叉熵损失可以简写成$L(\\theta)=-(1/n)\\sum_{i=1}^n\\log\\hat y^{(i)}_{y^{(i)}}$。从另一个角度来看，我们知道最小化$L(\\theta)$等价于最大化$exp(-nL(\\theta))=\\prod_{i=1}^n \\hat y^{(i)}_{y^{(i)}}$，即最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率。\n",
    "\n",
    "# 模型训练和预测\n",
    "- 在训练好softmax回归模型后，给定任一样本特征，就可以预测每个输出类别的概率。通常，我们把预测概率最大的类别作为输出类别。如果它与真实类别（标签）一致，说明这次预测是正确的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "90a2cc37-7e59-4dc5-b5ec-d2b70da15776"
   },
   "source": [
    "## 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义数据集\n",
    "- 自定义输入X为7张高和宽均为2像素的灰度图片\n",
    "- 自定义输出target为$y_1=0$, $y_2=1$, $y_3=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "e0cfc887-f65f-4a0d-9147-f30384b1a94b"
   },
   "outputs": [],
   "source": [
    "# 确定随机数种子\n",
    "torch.manual_seed(7)\n",
    "# 自定义数据集\n",
    "X = torch.rand((7, 2, 2))\n",
    "target = torch.randint(0, 2, (7,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络结构\n",
    "- 定义如下所示的网络结构\n",
    "    - 一层全连接层 + Softmax层\n",
    "    - $x_1$,$x_2$,$x_3$,$x_4$为 X\n",
    "    - $o_1$,$o_2$,$o_3$为 target  \n",
    "\n",
    " <img src=\"https://cdn.nlark.com/yuque/0/2021/jpeg/1508544/1613313946469-ca66b5b1-4365-404d-9777-e7bf9abf890c.jpeg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义网络结构\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        # 定义一层全连接层\n",
    "        self.dense = nn.Linear(4, 3)\n",
    "        # 定义Softmax\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.dense(x.view((-1, 4)))\n",
    "        y = self.softmax(y)\n",
    "        return y\n",
    "\n",
    "net = LinearNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "f080cfd0-40a3-4dfe-a66c-3a1bac62b50b"
   },
   "source": [
    "## 定义损失函数\n",
    "- torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n",
    "- 衡量模型输出与真实标签的差异，在分类时相当有用。\n",
    "- 结合了nn.LogSoftmax()和nn.NLLLoss()两个函数，进行交叉熵计算。\n",
    "- 主要参数：\n",
    "    - weight: 各类别的loss设置权值\n",
    "    - ignore_index: 忽略某个类别\n",
    "    - reduction: 计算模式，可为none/sum/mean\n",
    "        - none: 逐个元素计算\n",
    "        - sum: 所有元素求和，返回标量\n",
    "        - mean: 加权平均，返回标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()  # 交叉熵损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义优化函数\n",
    "- torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "- 构建一个优化器对象optimizer，用来保存当前的状态，并能够根据计算得到的梯度来更新参数，使得模型输出更接近真实标签。\n",
    "- 学习率（learning rate）控制更新的步伐。\n",
    "- 主要参数：\n",
    "    - params: 管理的参数组\n",
    "    - lr: 初始化学习率\n",
    "    - momentum: 动量系数\n",
    "    - weight_decay: L2正则化系数\n",
    "    - nesterov: 是否采用NAG\n",
    "- zero_grad(): 清空所管理参数的梯度，因为Pytorch张量梯度不自动清零。\n",
    "- step(): 执行一步更新\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "uuid": "d6966c7b-6d59-4b6d-80f2-4d6b0f8ef80d"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)  # 随机梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "858540ac-3359-4aaf-a5c2-6ee534309b3e"
   },
   "source": [
    "## 开始训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "uuid": "f1e1eeb1-9472-41a3-8305-0064c35b255e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1055\n",
      "epoch 2, loss 1.0455\n",
      "epoch 3, loss 1.0058\n",
      "epoch 4, loss 0.9789\n",
      "epoch 5, loss 0.9587\n",
      "epoch 6, loss 0.9419\n",
      "epoch 7, loss 0.9269\n",
      "epoch 8, loss 0.9130\n",
      "epoch 9, loss 0.8999\n",
      "epoch 10, loss 0.8874\n",
      "epoch 11, loss 0.8752\n",
      "epoch 12, loss 0.8634\n",
      "epoch 13, loss 0.8519\n",
      "epoch 14, loss 0.8407\n",
      "epoch 15, loss 0.8298\n",
      "epoch 16, loss 0.8194\n",
      "epoch 17, loss 0.8093\n",
      "epoch 18, loss 0.7995\n",
      "epoch 19, loss 0.7901\n",
      "epoch 20, loss 0.7810\n",
      "epoch 21, loss 0.7723\n",
      "epoch 22, loss 0.7639\n",
      "epoch 23, loss 0.7559\n",
      "epoch 24, loss 0.7482\n",
      "epoch 25, loss 0.7409\n",
      "epoch 26, loss 0.7338\n",
      "epoch 27, loss 0.7270\n",
      "epoch 28, loss 0.7206\n",
      "epoch 29, loss 0.7145\n",
      "epoch 30, loss 0.7086\n",
      "epoch 31, loss 0.7030\n",
      "epoch 32, loss 0.6977\n",
      "epoch 33, loss 0.6926\n",
      "epoch 34, loss 0.6878\n",
      "epoch 35, loss 0.6832\n",
      "epoch 36, loss 0.6788\n",
      "epoch 37, loss 0.6747\n",
      "epoch 38, loss 0.6707\n",
      "epoch 39, loss 0.6669\n",
      "epoch 40, loss 0.6632\n",
      "epoch 41, loss 0.6598\n",
      "epoch 42, loss 0.6565\n",
      "epoch 43, loss 0.6533\n",
      "epoch 44, loss 0.6503\n",
      "epoch 45, loss 0.6474\n",
      "epoch 46, loss 0.6447\n",
      "epoch 47, loss 0.6421\n",
      "epoch 48, loss 0.6396\n",
      "epoch 49, loss 0.6372\n",
      "epoch 50, loss 0.6349\n",
      "epoch 51, loss 0.6327\n",
      "epoch 52, loss 0.6306\n",
      "epoch 53, loss 0.6286\n",
      "epoch 54, loss 0.6267\n",
      "epoch 55, loss 0.6249\n",
      "epoch 56, loss 0.6232\n",
      "epoch 57, loss 0.6215\n",
      "epoch 58, loss 0.6199\n",
      "epoch 59, loss 0.6183\n",
      "epoch 60, loss 0.6168\n",
      "epoch 61, loss 0.6154\n",
      "epoch 62, loss 0.6140\n",
      "epoch 63, loss 0.6127\n",
      "epoch 64, loss 0.6115\n",
      "epoch 65, loss 0.6102\n",
      "epoch 66, loss 0.6091\n",
      "epoch 67, loss 0.6079\n",
      "epoch 68, loss 0.6068\n",
      "epoch 69, loss 0.6058\n",
      "epoch 70, loss 0.6047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(70):\n",
    "    train_l = 0.0\n",
    "    y_hat = net(X)\n",
    "    l = loss(y_hat, target).sum()\n",
    "\n",
    "    # 梯度清零\n",
    "    optimizer.zero_grad()\n",
    "    # 自动求导梯度\n",
    "    l.backward()\n",
    "    # 利用优化函数调整所有权重参数\n",
    "    optimizer.step()\n",
    "\n",
    "    train_l += l\n",
    "    print('epoch %d, loss %.4f' % (epoch + 1, train_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "14e0a688-cdb0-424e-855f-e8ff5cae3aec"
   },
   "source": [
    "# 练习题\n",
    "选择题：\n",
    "1. `softmax([100, 101, 102])`的结果等于以下的哪一项\n",
    "    - A.`softmax([10.0, 10.1, 10.2])`\n",
    "    - B.`softmax([-100, -101, -102])`\n",
    "    - C.`softmax([-2, -1, 0])`\n",
    "    - D.`softmax([1000, 1010, 1020])` \n",
    "\n",
    "答案：\n",
    "1. C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "468b3367-3241-4a5b-8d93-0a1d606b5f27"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [],
   "description": "",
   "notebookId": "163327",
   "source": "dsw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
