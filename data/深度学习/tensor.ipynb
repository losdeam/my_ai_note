{"cells":[{"cell_type":"markdown","metadata":{},"source":["[前往页底](#end)\n","- 在深度学习中，我们通常会频繁地对数据进行操作。我们初窥张量，要学会如何去创建、操作张量。建议不要一开始就花费大量时间来研究明白API的作用，可以结合最后的线性回归模型等实战来学习。\n","# 张量的创建\n","- 张量（Tensors）类似于NumPy的ndarrays（点[NumPy实践](https://tianchi.aliyun.com/course/323?spm=5176.21206777.J_7877492310.2.569817c9hxzgeE)了解），但张量可以在GPU上进行计算。\n","所以从本质上来说，PyTorch是一个处理张量的库。一个张量是一个数字、向量、矩阵或任何n维数组。  \n","- 下面分别展示了0维张量到n位张量：\n","<img src=\"https://cdn.nlark.com/yuque/0/2021/png/1508544/1616380696196-0db578f8-055f-479b-b647-0e70bde55c22.png\"/>"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7f168427f780>"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import numpy\n","torch.manual_seed(7) # 固定随机数种子"]},{"cell_type":"markdown","metadata":{},"source":["## 一、直接创建\n","1. torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False)\n","- 功能：从data创建tensor\n","    - data: 数据，可以是list，numpy\n","    - dtype: 数据类型，默认与data的一致\n","    - device: 所在设备，cuda/cpu\n","    - requires_grad: 是否需要梯度\n","    - pin_memory: 是否存于锁页内存"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[0.1000, 1.2000],\n","        [2.2000, 3.1000],\n","        [4.9000, 5.2000]])"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])"]},{"cell_type":"markdown","metadata":{},"source":["2. torch.from_numpy(ndarray)\n","- 功能：从numpy创建tensor\n","- 注意事项：从torch.from_numpy创建的tensor于原ndarray共享内存，当修改其中一个数据，另一个也将会被改动。"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["a = numpy.array([1, 2, 3])\n","t = torch.from_numpy(a)"]},{"cell_type":"markdown","metadata":{},"source":["## 二、依据数值创建\n","1. torch.zeros(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n","- 功能：依size创建全0张量\n","    - size: 张量的形状，如(3, 3)、(3, 224, 224)\n","    - out: 输出的张量\n","    - layout: 内存中布局形式，有strided, sparse_coo等\n","    - device: 所在设备，gpu/cpu\n","    - requires_grad: 是否需要梯度"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[0., 0., 0.],\n","        [0., 0., 0.]])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["torch.zeros(2, 3)"]},{"cell_type":"markdown","metadata":{},"source":["2. torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False)\n","- 功能：依input形状创建全0张量\n","    - input: 创建与input同形状的全0张量\n","    - dtype: 数据类型\n","    - layout: 内存中布局形式"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[0., 0., 0.],\n","        [0., 0., 0.]])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["input = torch.empty(2, 3)\n","torch.zeros_like(input)"]},{"cell_type":"markdown","metadata":{},"source":["3. torch.ones(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[1., 1., 1.],\n","        [1., 1., 1.]])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["torch.ones(2, 3)"]},{"cell_type":"markdown","metadata":{},"source":["4. torch.ones_like(input, dtype=None, layout=None, device=None, requires_grad=False)\n","- 功能：依input形状创建全1张量\n","    - size: 张量的形状，如(3, 3)、(3, 224, 224)\n","    - dtype: 数据类型\n","    - layout: 内存中布局形式\n","    - device: 所在设备，gpu/cpu\n","    - requires_grad: 是否需要梯度"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[1., 1., 1.],\n","        [1., 1., 1.]])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["input = torch.empty(2, 3)\n","torch.ones_like(input)"]},{"cell_type":"markdown","metadata":{},"source":["5. torch.full(size, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[3.1416, 3.1416, 3.1416],\n","        [3.1416, 3.1416, 3.1416]])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["torch.full((2, 3), 3.141592)"]},{"cell_type":"markdown","metadata":{},"source":["6. torch.full_like(input, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n","- 功能: 依input形状创建指定数据的张量\n","    - size: 张量的形状，如(3, 3)\n","    - fill_value: 张量的值\n","7. torch.arange(start=0, end. step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n","- 功能：创建等差的1维张量\n","    - start: 数列起始值\n","    - end: 数列“结束值”\n","    - step: 数列公差，默认为1\n","- 注意事项：数值区间为$[start, end)$"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([1.0000, 1.5000, 2.0000])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["torch.arange(1, 2.5, 0.5)"]},{"cell_type":"markdown","metadata":{},"source":["8. torch.linspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n","- 功能：创建均分的1维张量\n","    - start: 数列起始值\n","    - end: 数列结束值\n","    - steps: 数列长度"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([-10.,  -5.,   0.,   5.,  10.])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["torch.linspace(start=-10, end=10, steps=5)"]},{"cell_type":"markdown","metadata":{},"source":["9. torch.logspace(start, end, steps=100, base=10.0, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n","- 功能: 创建对数均分的1维张量\n","- 注意事项：长度为steps，底为base\n","    - start: 数列起始值\n","    - end: 数列结束值\n","    - steps: 数列长度\n","    - base: 对数函数的底，默认为10"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([ 1.2589,  2.1135,  3.5481,  5.9566, 10.0000])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["torch.logspace(start=0.1, end=1.0, steps=5)"]},{"cell_type":"markdown","metadata":{},"source":["10. torch.eye(n, m=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n","- 功能: 创建单位对角矩阵（2维张量）\n","- 注意事项：默认为方阵\n","    - n：矩阵行数\n","    - m：矩阵列数"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.]])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["torch.eye(3)"]},{"cell_type":"markdown","metadata":{},"source":["## 三、依概率分布创建张量\n","1. torch.normal(mean, std, out=None)\n","- 功能：生成正态分布（高斯分布）\n","    - mean: 均值\n","    - std: 标准差\n","- 四种模式：\n","    - mean为标量，std为标量\n","    - mean为标量，std为张量\n","    - mean为张量，std为标量\n","    - mean为张量，std为张量"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([0.8532, 2.7075, 3.7575, 3.2200, 6.0145, 5.5526, 6.8577, 8.3697, 9.0276,\n","        9.8318])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# mean为张量, std为张量\n","torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))"]},{"cell_type":"markdown","metadata":{},"source":["2. torch.normal(mean, std, size, out=None)\n","• 功能：生成一定大小的生成正态分布（高斯分布）"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[2.9530, 2.3984, 2.4120, 2.7216]])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["torch.normal(2, 3, size=(1, 4))"]},{"cell_type":"markdown","metadata":{},"source":["3. torch.randn(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n","- 功能：生成标准正态分布\n","    - size: 张量的形状"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[1.3955, 1.3470, 2.4382],\n","        [0.2028, 2.4505, 2.0256]])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["torch.randn(2, 3)"]},{"cell_type":"markdown","metadata":{},"source":["4. torch.rand(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n","- 功能：在区间$[0, 1)$上，生成均匀分布"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[0.7405, 0.2529, 0.2332],\n","        [0.9314, 0.9575, 0.5575]])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["torch.rand(2, 3)"]},{"cell_type":"markdown","metadata":{},"source":["5. torch.randint(low=0, high, size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n","- 功能：区间 $[low, high)$ 生成整数均匀分布\n","    - size：张量的形状"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[4, 8],\n","        [7, 8]])"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["torch.randint(3, 10, (2, 2))"]},{"cell_type":"markdown","metadata":{},"source":["6. torch.randperm(n, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False)\n","- 功能：生成从0到n-1的随机排列\n","    - n：张量的长度"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([1, 3, 0, 2])"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["torch.randperm(4)"]},{"cell_type":"markdown","metadata":{},"source":["7. torch.bernoulli(input, *, generator=None, out=None)\n","- 功能：以input为概率，生成伯努利分布（0-1分布，两点分布）\n","    - input：概率值"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[0., 0., 1.],\n","        [0., 1., 1.],\n","        [1., 0., 1.]])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["a = torch.empty(3, 3).uniform_(0, 1)\n","torch.bernoulli(a)"]},{"cell_type":"markdown","metadata":{},"source":["# 张量的操作\n","## 一、张量拼接与切分\n","1. torch.cat(tensors, dim=0, out=None)\n","- 功能：将张量按维度dim进行拼接\n","    - tensors：张量序列\n","    - dim：要拼接的维度"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[-1.7038,  0.6248,  0.1196, -1.7038,  0.6248,  0.1196, -1.7038,  0.6248,\n","          0.1196],\n","        [-0.8049,  1.6162,  0.2516, -0.8049,  1.6162,  0.2516, -0.8049,  1.6162,\n","          0.2516]])"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["x = torch.randn(2, 3)\n","torch.cat((x, x, x), 1)"]},{"cell_type":"markdown","metadata":{},"source":["2. torch.stack(tensors, dim=0, out=None)\n","- 功能：在新创建的维度dim上进行拼接\n","    - tensors：张量序列\n","    - dim：要拼接的维度\n","3. torch.chunk(input, chunks, dim=0)\n","- 功能：将张量按维度dim进行平均切分\n","- 返回值：张量列表\n","- 注意事项：若不能整除，最后一份张量小于其他张量\n","    - input：要切分的张量\n","    - chunks：要切分的份数\n","    - dim：要切分的维度\n","4. torch.split(tensor, split_size_or_sections, dim=0)\n","- 功能：将张量按维度dim进行切分\n","- 返回值：张量列表\n","    - tensor：要切分的张量\n","    - split_size_or_sections：为int时，表示每一份的长度；为list时，按list元素切分\n","    - dim：要切分的维度"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["(tensor([[0, 1],\n","         [2, 3]]), tensor([[4, 5],\n","         [6, 7]]), tensor([[8, 9]]))"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["a = torch.arange(10).reshape(5,2)\n","torch.split(a, 2)"]},{"cell_type":"markdown","metadata":{},"source":["## 二、张量索引\n","1. torch.index_select(input, dim, index, out=None)\n","- 功能：在维度dim上，按index索引数据\n","- 返回值：依index索引数据拼接的张量\n","    - index：要索引的张量\n","    - dim：要索引的维度\n","    - index：要索引数据的序号"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[-0.0510,  0.1323,  0.3916,  1.0830],\n","        [ 0.3809,  0.2569, -1.0273,  0.4999]])"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["x = torch.randn(3, 4)\n","indices = torch.tensor([0, 2])\n","torch.index_select(x, 0, indices)"]},{"cell_type":"markdown","metadata":{},"source":["2. torch.masked_select(input, mask, out=None)\n","- 功能：按mask中的True进行索引\n","- 返回值：一维张量\n","    - input：要索引的张量\n","    - mask：与input同形状的布尔类型张量"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([0.5054, 0.8079])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["x = torch.randn(3, 4)\n","mask = x.ge(0.5)\n","torch.masked_select(x, mask)"]},{"cell_type":"markdown","metadata":{},"source":["## 三、张量变换\n","1. torch.reshape(input, shape)\n","- 功能：变换张量形状\n","- 注意事项：当张量在内存中是连续时，新张量与input共享数据内存\n","    - input：要变换的张量\n","    - shape：新张量的形状"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[0., 1.],\n","        [2., 3.]])"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["a = torch.arange(4.)\n","torch.reshape(a, (2, 2))"]},{"cell_type":"markdown","metadata":{},"source":["2. torch.transpose(input, dim0, dim1)\n","- 功能：交换张量的两个维度\n","    - input：要交换的张量\n","    - dim0：要交换的维度\n","    - dim1：要交换的维度"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[ 0.0096,  0.4506],\n","        [-0.5704,  0.4101],\n","        [-0.1722,  0.8957]])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["x = torch.randn(2, 3)\n","torch.transpose(x, 0, 1)"]},{"cell_type":"markdown","metadata":{},"source":["3. torch.t(input)\n","- 功能：2维张量转置，对矩阵而言，等价于torch.transpose(input, 0, 1)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(-0.2882)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["x = torch.randn(())\n","torch.t(x)"]},{"cell_type":"markdown","metadata":{},"source":["4. torch.squeeze(input, dim=None, out=None)\n","- 功能：压缩长度为1的维度（轴）\n","    - dim：若为None，移除所有长度为1的轴；若指定维度，当且仅当该轴长度为1时，可以被移除"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["x = torch.zeros(2, 1, 2, 1, 2)\n","y = torch.squeeze(x)"]},{"cell_type":"markdown","metadata":{},"source":["5. torch.unsqueeze(input, dim, out=None)\n","- 功能：依据dim扩展维度\n","    - dim：扩展的维度"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[1, 2, 3, 4]])"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["x = torch.tensor([1, 2, 3, 4])\n","torch.unsqueeze(x, 0)"]},{"cell_type":"markdown","metadata":{},"source":["## 线性回归模型\n","- 线性回归是分析一个变量与另外一（多）个变量之间关系的方法。\n","    - 因变量是$y$，自变量是$x$，关系线性：$y=w\\times x + b$，任务是求解 $w$，$b$。\n","- 我们的求解步骤是：\n","- 确定模型：$Model \\to y = w \\times x + b$\n","- 选择损失函数：这里用$MSE:\\frac{1}{m}\\sum_{i=1}^m(y_i-\\hat y_i)^2$\n","- 求解梯度并更新$w$，$b$：\n","$$\n","\\begin{array}{lcl}\n","w &=& w - lr \\times w.grad \\\\\n","b &=& b - lr \\times w.grad\n","\\end{array}\n","$$\n","- 下面我们开始写一个线性回归模型："]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([-1.0229e+36]) tensor([-1.6542e+35])\n"]}],"source":["# 首先我们得有训练样本X，Y， 这里我们随机生成\n","x = torch.rand(20, 1) * 10\n","y = 2 * x + (5 + torch.randn(20, 1))\n","\n","# 构建线性回归函数的参数\n","w = torch.randn((1), requires_grad=True)\n","b = torch.zeros((1), requires_grad=True)   # 这俩都需要求梯度\n","\n","# 设置学习率lr为0.1\n","lr = 0.1\n","\n","for iteration in range(100):\n","    # 前向传播\n","    wx = torch.mul(w, x)\n","    y_pred = torch.add(wx, b)\n"," \n","    # 计算loss\n","    loss = (0.5 * (y-y_pred)**2).mean()\n"," \n","    # 反向传播\n","    loss.backward()\n"," \n","    # 更新参数\n","    b.data.sub_(lr * b.grad)    # 这种_的加法操作时从自身减，相当于-=\n","    w.data.sub_(lr * w.grad)\n","\n","    # 梯度清零\n","    w.grad.data.zero_()\n","    b.grad.data.zero_()\n","\n","print(w.data, b.data)"]},{"cell_type":"markdown","metadata":{},"source":["# 练习题\n","通过torch.normal创建四种模式的正态分布张量。"]},{"cell_type":"markdown","metadata":{},"source":["<p id = \"end\"><p>\n","\n","[返回深度学习笔记](../../深度学习.md)\n","\n","[直接前往张量操作部分]()"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"tianchi_metadata":{"competitions":[],"datasets":[],"description":"","notebookId":"160676","source":"dsw"}},"nbformat":4,"nbformat_minor":4}
