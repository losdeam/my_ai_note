{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "8335625d-4a70-4736-9cd3-e4ba01ce393e"
   },
   "source": [
    "- 在深度学习模型日益庞大的今天，并非所有人都能满足从头开始训练一个模型的软硬件条件，稀缺的数据和昂贵的计算资源都是我们需要面对的难题。迁移学习可以帮助我们缓解在数据和计算资源上的尴尬。作为当前深度学习领域中最重要的方法论之一，迁移学习有着自己自身的理论依据和实际效果验证。\n",
    "- 作为一门实验性学科，深度学习通常需要反复的实验和结果论证。在现在和将来，是否有海量的数据资源和强大的计算资源，这是决定学界和业界深度学习和人工智能发展的关键因素。通常情况下，获取海量的数据资源对于企业而言并非易事，尤其是对于像医疗等特定领域，要想做一个基于深度学习的医学影像的辅助诊断系统，大量且高质量的打标数据非常关键。但通常而言，不要说高质量，就是想获取大量的医疗数据就已困难重重。\n",
    "- 那怎么办呢？是不是获取不了海量的数据研究就一定进行不下去了？当然不是。因为我们有迁移学习。那究竟什么是迁移学习？顾名思义，迁移学习就是利用数据、任务或模型之间的相似性，将在旧的领域学习过或训练好的模型，应用于新的领域这样的一个过程。从这段定义里面，我们可以窥见迁移学习的关键点所在，即新的任务与旧的任务在数据、任务和模型之间的相似性。\n",
    "- 本节我们介绍迁移学习中的一种常用技术：微调（fine tuning）。微调由以下4步构成。\n",
    "1. 在源数据集（如ImageNet数据集）上预训练一个神经网络模型，即源模型。\n",
    "2. 创建一个新的神经网络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于目标数据集。我们还假设源模型的输出层跟源数据集的标签紧密相关，因此在目标模型中不予采用。\n",
    "3. 为目标模型添加一个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。\n",
    "4. 在目标数据集（如FashionMNIST数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。\n",
    "<center/><img src=\"https://cdn.nlark.com/yuque/0/2021/jpeg/1508544/1614151636690-a8025370-aa39-48e5-a34e-e2fad2816cde.jpeg\"/></center>\n",
    "- 当目标数据集远小于源数据集时，微调有助于提升模型的泛化能力。\n",
    "# 加载数据集\n",
    "- FashionMNIST是28×28的灰度图片，60000/10000的训练测试数据划分，其涵盖了来自10种类别的共7万个不同商品的正面图片。\n",
    "<center/><img src=\"https://cdn.nlark.com/yuque/0/2021/png/1508544/1614153298387-a8d9fcaa-119a-4b0a-ac5e-b5f18478e077.png\"/></center>\n",
    "- 假设我们想从图像中识别出不同种类的衣物。一种可能的方法是先收集尽可能多的衣物的不同拍摄角度的图片，然后在收集到的图像数据集上训练一个分类模型，但样本数仍然不及ImageNet数据集中样本数的十分之一。这可能会导致适用于ImageNet数据集的复杂模型在这个椅子数据集上过拟合。同时，因为数据量有限，最终训练得到的模型的精度也可能达不到实用的要求。\n",
    "- 为了应对上述问题，一个显而易见的解决办法是收集更多的数据。然而，收集和标注数据会花费大量的时间和资金。例如，为了收集ImageNet数据集，研究人员花费了数百万美元的研究经费。虽然目前的数据采集成本已降低了不少，但其成本仍然不可忽略。\n",
    "- 另外一种解决办法是应用迁移学习（transfer learning），将从源数据集学到的知识迁移到目标数据集上。例如，虽然ImageNet数据集的图像大多跟衣物无关，但在该数据集上训练的模型可以抽取较通用的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于识别衣物也可能同样有效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意：fashion mnist数据集加载，需要先安装ipywidgets，不然会报错提示需要升级jupyter\n",
    "pip install ipywidgets --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "uuid": "2a6f0424-594c-4924-8b55-557269547f94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./Datasets/FashionMNIST\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.037088632583618164,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 26421880,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828a875e5bbd4e018406b48717f3cb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Datasets/FashionMNIST\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./Datasets/FashionMNIST\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./Datasets/FashionMNIST\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021472454071044922,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 29515,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262801dce1ee4ebcb881b397e8e4b5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Datasets/FashionMNIST\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./Datasets/FashionMNIST\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./Datasets/FashionMNIST\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.027331113815307617,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4422102,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2475eb12b3744b919cf7b810d4dc1720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Datasets/FashionMNIST\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./Datasets/FashionMNIST\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./Datasets/FashionMNIST\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02245187759399414,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5148,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2a508ec47a4624b5807bf48f5426e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Datasets/FashionMNIST\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./Datasets/FashionMNIST\\FashionMNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 在CPU版本自动下载数据\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "###################fashion mnist数据集加载######################\n",
    "def load_data_fashion_mnist(batch_size, resize=None, root='./Datasets/FashionMNIST'):\n",
    "    \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\n",
    "    trans = []\n",
    "    if resize:\n",
    "        trans.append(torchvision.transforms.Resize(size=resize))\n",
    "    trans.append(torchvision.transforms.ToTensor())\n",
    "    \n",
    "    \n",
    "    transform = torchvision.transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n",
    "    \n",
    "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_iter, test_iter\n",
    "#################################################################\n",
    "\n",
    "batch_size = 64\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "7da02b91-af7c-4dfd-9bde-72d391aee8c4"
   },
   "source": [
    "# 模型使用\n",
    "Pytorchvision支持多种图像分类模型，这里我们选择残差网络模型作为迁移学习的基础模型，对输出层（最后一层）改为十个类别，其它特征层选择在训练时候微调参数。常见的ResNet网络模型如下：\n",
    "<img src=\"https://cdn.nlark.com/yuque/0/2021/png/1508544/1614152398516-530cf2a8-0eb7-4277-887f-c701e2b24011.png\"/>\n",
    "基于ResNet18完成网络模型修改，最终的模型实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "uuid": "70ff4d29-b259-4f33-adf1-a9dd6abe983b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SurfaceDefectResNet(\n",
      "  (cnn_layers): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 在CPU版本自动下载模型参数\n",
    "class SurfaceDefectResNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SurfaceDefectResNet, self).__init__()\n",
    "        # Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/admin/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
    "        self.cnn_layers = torchvision.models.resnet18(pretrained=True)\n",
    "        num_ftrs = self.cnn_layers.fc.in_features\n",
    "        self.cnn_layers.fc = torch.nn.Linear(num_ftrs, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "\n",
    "        # stack convolution layers\n",
    "        out = self.cnn_layers(x)\n",
    "        return out\n",
    "\n",
    "net = SurfaceDefectResNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "822b3f83-a5ae-4b67-adee-c011ccc4bee3"
   },
   "source": [
    "# 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "uuid": "6205a198-dd41-4421-8366-77abc5ec3ad9"
   },
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "71238e65-06f2-4979-9411-b5005984648f"
   },
   "source": [
    "# 定义优化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "uuid": "3b2c0519-b31c-41a2-9399-b54bed03915c"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "83bcc538-3116-4eb0-b2e5-4013952e0442"
   },
   "source": [
    "# 定义准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "uuid": "31277a5f-01fa-4e70-b7d5-2d53283d51df"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    return (y_hat.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        y = y\n",
    "        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "72313610-9172-4b7d-828e-cce0cc3b0335"
   },
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "uuid": "87dd8420-9aec-41d6-b232-79e16147ed3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[64, 1, 7, 7] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\WINDOWS\\TEMP/ipykernel_4388/2902234663.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m               % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n\u001b[0;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mtrain_ch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\WINDOWS\\TEMP/ipykernel_4388/2902234663.py\u001b[0m in \u001b[0;36mtrain_ch\u001b[1;34m(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr, optimizer)\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mtrain_acc_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n\u001b[0;32m     32\u001b[0m               % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
      "\u001b[1;32mC:\\WINDOWS\\TEMP/ipykernel_4388/602083125.py\u001b[0m in \u001b[0;36mevaluate_accuracy\u001b[1;34m(data_iter, net)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0macc_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0macc_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0macc_sum\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1109\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1110\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1112\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WINDOWS\\TEMP/ipykernel_4388/2186419618.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# stack convolution layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnn_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1109\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1110\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1112\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\d2l\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\d2l\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;31m# See note [TorchScript super()]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1109\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1110\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1112\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    441\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 443\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    444\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[64, 1, 7, 7] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "def train_ch(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, optimizer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch)\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        t = 1 \n",
    "        for X, y in train_iter:\n",
    "            print(t)\n",
    "            t+=1\n",
    "            X = torch.cat((X, X, X), 1)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).sum()\n",
    "            # print(1)\n",
    "            # 梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            # print(2)\n",
    "            l.backward()\n",
    "            # print(3)\n",
    "            optimizer.step() \n",
    "            # print(4)\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "print(len(train_iter))\n",
    "train_ch(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[64, 1, 7, 7] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\WINDOWS\\TEMP/ipykernel_4388/2749342368.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m               % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n\u001b[0;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mtrain_ch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\WINDOWS\\TEMP/ipykernel_4388/2749342368.py\u001b[0m in \u001b[0;36mtrain_ch\u001b[1;34m(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr, optimizer)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mtrain_acc_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n\u001b[0;32m     36\u001b[0m               % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
      "\u001b[1;32mC:\\WINDOWS\\TEMP/ipykernel_4388/602083125.py\u001b[0m in \u001b[0;36mevaluate_accuracy\u001b[1;34m(data_iter, net)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0macc_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0macc_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0macc_sum\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1109\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1110\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1112\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WINDOWS\\TEMP/ipykernel_4388/2186419618.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# stack convolution layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnn_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1109\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1110\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1112\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\d2l\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\d2l\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;31m# See note [TorchScript super()]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1109\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1110\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1112\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\conda\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    441\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 443\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    444\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[64, 1, 7, 7] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = net.to(device)\n",
    "loss.to(device)\n",
    "def train_ch(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, optimizer=None):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch)\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        t = 1 \n",
    "        for X, y in train_iter:\n",
    "            print(t)\n",
    "            t+=1\n",
    "            X = torch.cat((X, X, X), 1).to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).sum()\n",
    "            # print(1)\n",
    "            # 梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            # print(2)\n",
    "            l.backward()\n",
    "            # print(3)\n",
    "            optimizer.step() \n",
    "            # print(4)\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "print(len(train_iter))\n",
    "train_ch(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先上面两串代码可以很明显的看出来，在调用GPU进行训练的情况下，训练的速度快了不止一点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后就是目前设计的resnet模型需要[64,3,7,7]的输入而我们的数据却是[64,1,7,7],有趣的是天池的代码中已经在训练过程中进行了数据格式的修正，而在验证的函数中却忘了这一点\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    return (y_hat.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        X = torch.cat((X, X, X), 1).to(device)\n",
    "        y = y.to(device)\n",
    "        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'epoch 1,0.1066  \n",
      "'epoch 1,0.2132  \n",
      "'epoch 1,0.3198  \n",
      "'epoch 1,0.4264  \n",
      "'epoch 1,0.5330  \n",
      "'epoch 1,0.6397  \n",
      "'epoch 1,0.7463  \n",
      "'epoch 1,0.8529  \n",
      "'epoch 1,0.9595  \n",
      "'epoch 1,1.0661  \n",
      "'epoch 1,1.1727  \n",
      "'epoch 1,1.2793  \n",
      "'epoch 1,1.3859  \n",
      "'epoch 1,1.4925  \n",
      "'epoch 1,1.5991  \n",
      "'epoch 1,1.7058  \n",
      "'epoch 1,1.8124  \n",
      "'epoch 1,1.9190  \n",
      "'epoch 1,2.0256  \n",
      "'epoch 1,2.1322  \n",
      "'epoch 1,2.2388  \n",
      "'epoch 1,2.3454  \n",
      "'epoch 1,2.4520  \n",
      "'epoch 1,2.5586  \n",
      "'epoch 1,2.6652  \n",
      "'epoch 1,2.7719  \n",
      "'epoch 1,2.8785  \n",
      "'epoch 1,2.9851  \n",
      "'epoch 1,3.0917  \n",
      "'epoch 1,3.1983  \n",
      "'epoch 1,3.3049  \n",
      "'epoch 1,3.4115  \n",
      "'epoch 1,3.5181  \n",
      "'epoch 1,3.6247  \n",
      "'epoch 1,3.7313  \n",
      "'epoch 1,3.8380  \n",
      "'epoch 1,3.9446  \n",
      "'epoch 1,4.0512  \n",
      "'epoch 1,4.1578  \n",
      "'epoch 1,4.2644  \n",
      "'epoch 1,4.3710  \n",
      "'epoch 1,4.4776  \n",
      "'epoch 1,4.5842  \n",
      "'epoch 1,4.6908  \n",
      "'epoch 1,4.7974  \n",
      "'epoch 1,4.9041  \n",
      "'epoch 1,5.0107  \n",
      "'epoch 1,5.1173  \n",
      "'epoch 1,5.2239  \n",
      "'epoch 1,5.3305  \n",
      "'epoch 1,5.4371  \n",
      "'epoch 1,5.5437  \n",
      "'epoch 1,5.6503  \n",
      "'epoch 1,5.7569  \n",
      "'epoch 1,5.8635  \n",
      "'epoch 1,5.9701  \n",
      "'epoch 1,6.0768  \n",
      "'epoch 1,6.1834  \n",
      "'epoch 1,6.2900  \n",
      "'epoch 1,6.3966  \n",
      "'epoch 1,6.5032  \n",
      "'epoch 1,6.6098  \n",
      "'epoch 1,6.7164  \n",
      "'epoch 1,6.8230  \n",
      "'epoch 1,6.9296  \n",
      "'epoch 1,7.0362  \n",
      "'epoch 1,7.1429  \n",
      "'epoch 1,7.2495  \n",
      "'epoch 1,7.3561  \n",
      "'epoch 1,7.4627  \n",
      "'epoch 1,7.5693  \n",
      "'epoch 1,7.6759  \n",
      "'epoch 1,7.7825  \n",
      "'epoch 1,7.8891  \n",
      "'epoch 1,7.9957  \n",
      "'epoch 1,8.1023  \n",
      "'epoch 1,8.2090  \n",
      "'epoch 1,8.3156  \n",
      "'epoch 1,8.4222  \n",
      "'epoch 1,8.5288  \n",
      "'epoch 1,8.6354  \n",
      "'epoch 1,8.7420  \n",
      "'epoch 1,8.8486  \n",
      "'epoch 1,8.9552  \n",
      "'epoch 1,9.0618  \n",
      "'epoch 1,9.1684  \n",
      "'epoch 1,9.2751  \n",
      "'epoch 1,9.3817  \n",
      "'epoch 1,9.4883  \n",
      "'epoch 1,9.5949  \n",
      "'epoch 1,9.7015  \n",
      "'epoch 1,9.8081  \n",
      "'epoch 1,9.9147  \n",
      "'epoch 1,10.0213  \n",
      "'epoch 1,10.1279  \n",
      "'epoch 1,10.2345  \n",
      "'epoch 1,10.3412  \n",
      "'epoch 1,10.4478  \n",
      "'epoch 1,10.5544  \n",
      "'epoch 1,10.6610  \n",
      "'epoch 1,10.7676  \n",
      "'epoch 1,10.8742  \n",
      "'epoch 1,10.9808  \n",
      "'epoch 1,11.0874  \n",
      "'epoch 1,11.1940  \n",
      "'epoch 1,11.3006  \n",
      "'epoch 1,11.4072  \n",
      "'epoch 1,11.5139  \n",
      "'epoch 1,11.6205  \n",
      "'epoch 1,11.7271  \n",
      "'epoch 1,11.8337  \n",
      "'epoch 1,11.9403  \n",
      "'epoch 1,12.0469  \n",
      "'epoch 1,12.1535  \n",
      "'epoch 1,12.2601  \n",
      "'epoch 1,12.3667  \n",
      "'epoch 1,12.4733  \n",
      "'epoch 1,12.5800  \n",
      "'epoch 1,12.6866  \n",
      "'epoch 1,12.7932  \n",
      "'epoch 1,12.8998  \n",
      "'epoch 1,13.0064  \n",
      "'epoch 1,13.1130  \n",
      "'epoch 1,13.2196  \n",
      "'epoch 1,13.3262  \n",
      "'epoch 1,13.4328  \n",
      "'epoch 1,13.5394  \n",
      "'epoch 1,13.6461  \n",
      "'epoch 1,13.7527  \n",
      "'epoch 1,13.8593  \n",
      "'epoch 1,13.9659  \n",
      "'epoch 1,14.0725  \n",
      "'epoch 1,14.1791  \n",
      "'epoch 1,14.2857  \n",
      "'epoch 1,14.3923  \n",
      "'epoch 1,14.4989  \n",
      "'epoch 1,14.6055  \n",
      "'epoch 1,14.7122  \n",
      "'epoch 1,14.8188  \n",
      "'epoch 1,14.9254  \n",
      "'epoch 1,15.0320  \n",
      "'epoch 1,15.1386  \n",
      "'epoch 1,15.2452  \n",
      "'epoch 1,15.3518  \n",
      "'epoch 1,15.4584  \n",
      "'epoch 1,15.5650  \n",
      "'epoch 1,15.6716  \n",
      "'epoch 1,15.7783  \n",
      "'epoch 1,15.8849  \n",
      "'epoch 1,15.9915  \n",
      "'epoch 1,16.0981  \n",
      "'epoch 1,16.2047  \n",
      "'epoch 1,16.3113  \n",
      "'epoch 1,16.4179  \n",
      "'epoch 1,16.5245  \n",
      "'epoch 1,16.6311  \n",
      "'epoch 1,16.7377  \n",
      "'epoch 1,16.8443  \n",
      "'epoch 1,16.9510  \n",
      "'epoch 1,17.0576  \n",
      "'epoch 1,17.1642  \n",
      "'epoch 1,17.2708  \n",
      "'epoch 1,17.3774  \n",
      "'epoch 1,17.4840  \n",
      "'epoch 1,17.5906  \n",
      "'epoch 1,17.6972  \n",
      "'epoch 1,17.8038  \n",
      "'epoch 1,17.9104  \n",
      "'epoch 1,18.0171  \n",
      "'epoch 1,18.1237  \n",
      "'epoch 1,18.2303  \n",
      "'epoch 1,18.3369  \n",
      "'epoch 1,18.4435  \n",
      "'epoch 1,18.5501  \n",
      "'epoch 1,18.6567  \n",
      "'epoch 1,18.7633  \n",
      "'epoch 1,18.8699  \n",
      "'epoch 1,18.9765  \n",
      "'epoch 1,19.0832  \n",
      "'epoch 1,19.1898  \n",
      "'epoch 1,19.2964  \n",
      "'epoch 1,19.4030  \n",
      "'epoch 1,19.5096  \n",
      "'epoch 1,19.6162  \n",
      "'epoch 1,19.7228  \n",
      "'epoch 1,19.8294  \n",
      "'epoch 1,19.9360  \n",
      "'epoch 1,20.0426  \n",
      "'epoch 1,20.1493  \n",
      "'epoch 1,20.2559  \n",
      "'epoch 1,20.3625  \n",
      "'epoch 1,20.4691  \n",
      "'epoch 1,20.5757  \n",
      "'epoch 1,20.6823  \n",
      "'epoch 1,20.7889  \n",
      "'epoch 1,20.8955  \n",
      "'epoch 1,21.0021  \n",
      "'epoch 1,21.1087  \n",
      "'epoch 1,21.2154  \n",
      "'epoch 1,21.3220  \n",
      "'epoch 1,21.4286  \n",
      "'epoch 1,21.5352  \n",
      "'epoch 1,21.6418  \n",
      "'epoch 1,21.7484  \n",
      "'epoch 1,21.8550  \n",
      "'epoch 1,21.9616  \n",
      "'epoch 1,22.0682  \n",
      "'epoch 1,22.1748  \n",
      "'epoch 1,22.2814  \n",
      "'epoch 1,22.3881  \n",
      "'epoch 1,22.4947  \n",
      "'epoch 1,22.6013  \n",
      "'epoch 1,22.7079  \n",
      "'epoch 1,22.8145  \n",
      "'epoch 1,22.9211  \n",
      "'epoch 1,23.0277  \n",
      "'epoch 1,23.1343  \n",
      "'epoch 1,23.2409  \n",
      "'epoch 1,23.3475  \n",
      "'epoch 1,23.4542  \n",
      "'epoch 1,23.5608  \n",
      "'epoch 1,23.6674  \n",
      "'epoch 1,23.7740  \n",
      "'epoch 1,23.8806  \n",
      "'epoch 1,23.9872  \n",
      "'epoch 1,24.0938  \n",
      "'epoch 1,24.2004  \n",
      "'epoch 1,24.3070  \n",
      "'epoch 1,24.4136  \n",
      "'epoch 1,24.5203  \n",
      "'epoch 1,24.6269  \n",
      "'epoch 1,24.7335  \n",
      "'epoch 1,24.8401  \n",
      "'epoch 1,24.9467  \n",
      "'epoch 1,25.0533  \n",
      "'epoch 1,25.1599  \n",
      "'epoch 1,25.2665  \n",
      "'epoch 1,25.3731  \n",
      "'epoch 1,25.4797  \n",
      "'epoch 1,25.5864  \n",
      "'epoch 1,25.6930  \n",
      "'epoch 1,25.7996  \n",
      "'epoch 1,25.9062  \n",
      "'epoch 1,26.0128  \n",
      "'epoch 1,26.1194  \n",
      "'epoch 1,26.2260  \n",
      "'epoch 1,26.3326  \n",
      "'epoch 1,26.4392  \n",
      "'epoch 1,26.5458  \n",
      "'epoch 1,26.6525  \n",
      "'epoch 1,26.7591  \n",
      "'epoch 1,26.8657  \n",
      "'epoch 1,26.9723  \n",
      "'epoch 1,27.0789  \n",
      "'epoch 1,27.1855  \n",
      "'epoch 1,27.2921  \n",
      "'epoch 1,27.3987  \n",
      "'epoch 1,27.5053  \n",
      "'epoch 1,27.6119  \n",
      "'epoch 1,27.7186  \n",
      "'epoch 1,27.8252  \n",
      "'epoch 1,27.9318  \n",
      "'epoch 1,28.0384  \n",
      "'epoch 1,28.1450  \n",
      "'epoch 1,28.2516  \n",
      "'epoch 1,28.3582  \n",
      "'epoch 1,28.4648  \n",
      "'epoch 1,28.5714  \n",
      "'epoch 1,28.6780  \n",
      "'epoch 1,28.7846  \n",
      "'epoch 1,28.8913  \n",
      "'epoch 1,28.9979  \n",
      "'epoch 1,29.1045  \n",
      "'epoch 1,29.2111  \n",
      "'epoch 1,29.3177  \n",
      "'epoch 1,29.4243  \n",
      "'epoch 1,29.5309  \n",
      "'epoch 1,29.6375  \n",
      "'epoch 1,29.7441  \n",
      "'epoch 1,29.8507  \n",
      "'epoch 1,29.9574  \n",
      "'epoch 1,30.0640  \n",
      "'epoch 1,30.1706  \n",
      "'epoch 1,30.2772  \n",
      "'epoch 1,30.3838  \n",
      "'epoch 1,30.4904  \n",
      "'epoch 1,30.5970  \n",
      "'epoch 1,30.7036  \n",
      "'epoch 1,30.8102  \n",
      "'epoch 1,30.9168  \n",
      "'epoch 1,31.0235  \n",
      "'epoch 1,31.1301  \n",
      "'epoch 1,31.2367  \n",
      "'epoch 1,31.3433  \n",
      "'epoch 1,31.4499  \n",
      "'epoch 1,31.5565  \n",
      "'epoch 1,31.6631  \n",
      "'epoch 1,31.7697  \n",
      "'epoch 1,31.8763  \n",
      "'epoch 1,31.9829  \n",
      "'epoch 1,32.0896  \n",
      "'epoch 1,32.1962  \n",
      "'epoch 1,32.3028  \n",
      "'epoch 1,32.4094  \n",
      "'epoch 1,32.5160  \n",
      "'epoch 1,32.6226  \n",
      "'epoch 1,32.7292  \n",
      "'epoch 1,32.8358  \n",
      "'epoch 1,32.9424  \n",
      "'epoch 1,33.0490  \n",
      "'epoch 1,33.1557  \n",
      "'epoch 1,33.2623  \n",
      "'epoch 1,33.3689  \n",
      "'epoch 1,33.4755  \n",
      "'epoch 1,33.5821  \n",
      "'epoch 1,33.6887  \n",
      "'epoch 1,33.7953  \n",
      "'epoch 1,33.9019  \n",
      "'epoch 1,34.0085  \n",
      "'epoch 1,34.1151  \n",
      "'epoch 1,34.2217  \n",
      "'epoch 1,34.3284  \n",
      "'epoch 1,34.4350  \n",
      "'epoch 1,34.5416  \n",
      "'epoch 1,34.6482  \n",
      "'epoch 1,34.7548  \n",
      "'epoch 1,34.8614  \n",
      "'epoch 1,34.9680  \n",
      "'epoch 1,35.0746  \n",
      "'epoch 1,35.1812  \n",
      "'epoch 1,35.2878  \n",
      "'epoch 1,35.3945  \n",
      "'epoch 1,35.5011  \n",
      "'epoch 1,35.6077  \n",
      "'epoch 1,35.7143  \n",
      "'epoch 1,35.8209  \n",
      "'epoch 1,35.9275  \n",
      "'epoch 1,36.0341  \n",
      "'epoch 1,36.1407  \n",
      "'epoch 1,36.2473  \n",
      "'epoch 1,36.3539  \n",
      "'epoch 1,36.4606  \n",
      "'epoch 1,36.5672  \n",
      "'epoch 1,36.6738  \n",
      "'epoch 1,36.7804  \n",
      "'epoch 1,36.8870  \n",
      "'epoch 1,36.9936  \n",
      "'epoch 1,37.1002  \n",
      "'epoch 1,37.2068  \n",
      "'epoch 1,37.3134  \n",
      "'epoch 1,37.4200  \n",
      "'epoch 1,37.5267  \n",
      "'epoch 1,37.6333  \n",
      "'epoch 1,37.7399  \n",
      "'epoch 1,37.8465  \n",
      "'epoch 1,37.9531  \n",
      "'epoch 1,38.0597  \n",
      "'epoch 1,38.1663  \n",
      "'epoch 1,38.2729  \n",
      "'epoch 1,38.3795  \n",
      "'epoch 1,38.4861  \n",
      "'epoch 1,38.5928  \n",
      "'epoch 1,38.6994  \n",
      "'epoch 1,38.8060  \n",
      "'epoch 1,38.9126  \n",
      "'epoch 1,39.0192  \n",
      "'epoch 1,39.1258  \n",
      "'epoch 1,39.2324  \n",
      "'epoch 1,39.3390  \n",
      "'epoch 1,39.4456  \n",
      "'epoch 1,39.5522  \n",
      "'epoch 1,39.6588  \n",
      "'epoch 1,39.7655  \n",
      "'epoch 1,39.8721  \n",
      "'epoch 1,39.9787  \n",
      "'epoch 1,40.0853  \n",
      "'epoch 1,40.1919  \n",
      "'epoch 1,40.2985  \n",
      "'epoch 1,40.4051  \n",
      "'epoch 1,40.5117  \n",
      "'epoch 1,40.6183  \n",
      "'epoch 1,40.7249  \n",
      "'epoch 1,40.8316  \n",
      "'epoch 1,40.9382  \n",
      "'epoch 1,41.0448  \n",
      "'epoch 1,41.1514  \n",
      "'epoch 1,41.2580  \n",
      "'epoch 1,41.3646  \n",
      "'epoch 1,41.4712  \n",
      "'epoch 1,41.5778  \n",
      "'epoch 1,41.6844  \n",
      "'epoch 1,41.7910  \n",
      "'epoch 1,41.8977  \n",
      "'epoch 1,42.0043  \n",
      "'epoch 1,42.1109  \n",
      "'epoch 1,42.2175  \n",
      "'epoch 1,42.3241  \n",
      "'epoch 1,42.4307  \n",
      "'epoch 1,42.5373  \n",
      "'epoch 1,42.6439  \n",
      "'epoch 1,42.7505  \n",
      "'epoch 1,42.8571  \n",
      "'epoch 1,42.9638  \n",
      "'epoch 1,43.0704  \n",
      "'epoch 1,43.1770  \n",
      "'epoch 1,43.2836  \n",
      "'epoch 1,43.3902  \n",
      "'epoch 1,43.4968  \n",
      "'epoch 1,43.6034  \n",
      "'epoch 1,43.7100  \n",
      "'epoch 1,43.8166  \n",
      "'epoch 1,43.9232  \n",
      "'epoch 1,44.0299  \n",
      "'epoch 1,44.1365  \n",
      "'epoch 1,44.2431  \n",
      "'epoch 1,44.3497  \n",
      "'epoch 1,44.4563  \n",
      "'epoch 1,44.5629  \n",
      "'epoch 1,44.6695  \n",
      "'epoch 1,44.7761  \n",
      "'epoch 1,44.8827  \n",
      "'epoch 1,44.9893  \n",
      "'epoch 1,45.0959  \n",
      "'epoch 1,45.2026  \n",
      "'epoch 1,45.3092  \n",
      "'epoch 1,45.4158  \n",
      "'epoch 1,45.5224  \n",
      "'epoch 1,45.6290  \n",
      "'epoch 1,45.7356  \n",
      "'epoch 1,45.8422  \n",
      "'epoch 1,45.9488  \n",
      "'epoch 1,46.0554  \n",
      "'epoch 1,46.1620  \n",
      "'epoch 1,46.2687  \n",
      "'epoch 1,46.3753  \n",
      "'epoch 1,46.4819  \n",
      "'epoch 1,46.5885  \n",
      "'epoch 1,46.6951  \n",
      "'epoch 1,46.8017  \n",
      "'epoch 1,46.9083  \n",
      "'epoch 1,47.0149  \n",
      "'epoch 1,47.1215  \n",
      "'epoch 1,47.2281  \n",
      "'epoch 1,47.3348  \n",
      "'epoch 1,47.4414  \n",
      "'epoch 1,47.5480  \n",
      "'epoch 1,47.6546  \n",
      "'epoch 1,47.7612  \n",
      "'epoch 1,47.8678  \n",
      "'epoch 1,47.9744  \n",
      "'epoch 1,48.0810  \n",
      "'epoch 1,48.1876  \n",
      "'epoch 1,48.2942  \n",
      "'epoch 1,48.4009  \n",
      "'epoch 1,48.5075  \n",
      "'epoch 1,48.6141  \n",
      "'epoch 1,48.7207  \n",
      "'epoch 1,48.8273  \n",
      "'epoch 1,48.9339  \n",
      "'epoch 1,49.0405  \n",
      "'epoch 1,49.1471  \n",
      "'epoch 1,49.2537  \n",
      "'epoch 1,49.3603  \n",
      "'epoch 1,49.4670  \n",
      "'epoch 1,49.5736  \n",
      "'epoch 1,49.6802  \n",
      "'epoch 1,49.7868  \n",
      "'epoch 1,49.8934  \n",
      "'epoch 1,50.0000  \n",
      "'epoch 1,50.1066  \n",
      "'epoch 1,50.2132  \n",
      "'epoch 1,50.3198  \n",
      "'epoch 1,50.4264  \n",
      "'epoch 1,50.5330  \n",
      "'epoch 1,50.6397  \n",
      "'epoch 1,50.7463  \n",
      "'epoch 1,50.8529  \n",
      "'epoch 1,50.9595  \n",
      "'epoch 1,51.0661  \n",
      "'epoch 1,51.1727  \n",
      "'epoch 1,51.2793  \n",
      "'epoch 1,51.3859  \n",
      "'epoch 1,51.4925  \n",
      "'epoch 1,51.5991  \n",
      "'epoch 1,51.7058  \n",
      "'epoch 1,51.8124  \n",
      "'epoch 1,51.9190  \n",
      "'epoch 1,52.0256  \n",
      "'epoch 1,52.1322  \n",
      "'epoch 1,52.2388  \n",
      "'epoch 1,52.3454  \n",
      "'epoch 1,52.4520  \n",
      "'epoch 1,52.5586  \n",
      "'epoch 1,52.6652  \n",
      "'epoch 1,52.7719  \n",
      "'epoch 1,52.8785  \n",
      "'epoch 1,52.9851  \n",
      "'epoch 1,53.0917  \n",
      "'epoch 1,53.1983  \n",
      "'epoch 1,53.3049  \n",
      "'epoch 1,53.4115  \n",
      "'epoch 1,53.5181  \n",
      "'epoch 1,53.6247  \n",
      "'epoch 1,53.7313  \n",
      "'epoch 1,53.8380  \n",
      "'epoch 1,53.9446  \n",
      "'epoch 1,54.0512  \n",
      "'epoch 1,54.1578  \n",
      "'epoch 1,54.2644  \n",
      "'epoch 1,54.3710  \n",
      "'epoch 1,54.4776  \n",
      "'epoch 1,54.5842  \n",
      "'epoch 1,54.6908  \n",
      "'epoch 1,54.7974  \n",
      "'epoch 1,54.9041  \n",
      "'epoch 1,55.0107  \n",
      "'epoch 1,55.1173  \n",
      "'epoch 1,55.2239  \n",
      "'epoch 1,55.3305  \n",
      "'epoch 1,55.4371  \n",
      "'epoch 1,55.5437  \n",
      "'epoch 1,55.6503  \n",
      "'epoch 1,55.7569  \n",
      "'epoch 1,55.8635  \n",
      "'epoch 1,55.9701  \n",
      "'epoch 1,56.0768  \n",
      "'epoch 1,56.1834  \n",
      "'epoch 1,56.2900  \n",
      "'epoch 1,56.3966  \n",
      "'epoch 1,56.5032  \n",
      "'epoch 1,56.6098  \n",
      "'epoch 1,56.7164  \n",
      "'epoch 1,56.8230  \n",
      "'epoch 1,56.9296  \n",
      "'epoch 1,57.0362  \n",
      "'epoch 1,57.1429  \n",
      "'epoch 1,57.2495  \n",
      "'epoch 1,57.3561  \n",
      "'epoch 1,57.4627  \n",
      "'epoch 1,57.5693  \n",
      "'epoch 1,57.6759  \n",
      "'epoch 1,57.7825  \n",
      "'epoch 1,57.8891  \n",
      "'epoch 1,57.9957  \n",
      "'epoch 1,58.1023  \n",
      "'epoch 1,58.2090  \n",
      "'epoch 1,58.3156  \n",
      "'epoch 1,58.4222  \n",
      "'epoch 1,58.5288  \n",
      "'epoch 1,58.6354  \n",
      "'epoch 1,58.7420  \n",
      "'epoch 1,58.8486  \n",
      "'epoch 1,58.9552  \n",
      "'epoch 1,59.0618  \n",
      "'epoch 1,59.1684  \n",
      "'epoch 1,59.2751  \n",
      "'epoch 1,59.3817  \n",
      "'epoch 1,59.4883  \n",
      "'epoch 1,59.5949  \n",
      "'epoch 1,59.7015  \n",
      "'epoch 1,59.8081  \n",
      "'epoch 1,59.9147  \n",
      "'epoch 1,60.0213  \n",
      "'epoch 1,60.1279  \n",
      "'epoch 1,60.2345  \n",
      "'epoch 1,60.3412  \n",
      "'epoch 1,60.4478  \n",
      "'epoch 1,60.5544  \n",
      "'epoch 1,60.6610  \n",
      "'epoch 1,60.7676  \n",
      "'epoch 1,60.8742  \n",
      "'epoch 1,60.9808  \n",
      "'epoch 1,61.0874  \n",
      "'epoch 1,61.1940  \n",
      "'epoch 1,61.3006  \n",
      "'epoch 1,61.4072  \n",
      "'epoch 1,61.5139  \n",
      "'epoch 1,61.6205  \n",
      "'epoch 1,61.7271  \n",
      "'epoch 1,61.8337  \n",
      "'epoch 1,61.9403  \n",
      "'epoch 1,62.0469  \n",
      "'epoch 1,62.1535  \n",
      "'epoch 1,62.2601  \n",
      "'epoch 1,62.3667  \n",
      "'epoch 1,62.4733  \n",
      "'epoch 1,62.5800  \n",
      "'epoch 1,62.6866  \n",
      "'epoch 1,62.7932  \n",
      "'epoch 1,62.8998  \n",
      "'epoch 1,63.0064  \n",
      "'epoch 1,63.1130  \n",
      "'epoch 1,63.2196  \n",
      "'epoch 1,63.3262  \n",
      "'epoch 1,63.4328  \n",
      "'epoch 1,63.5394  \n",
      "'epoch 1,63.6461  \n",
      "'epoch 1,63.7527  \n",
      "'epoch 1,63.8593  \n",
      "'epoch 1,63.9659  \n",
      "'epoch 1,64.0725  \n",
      "'epoch 1,64.1791  \n",
      "'epoch 1,64.2857  \n",
      "'epoch 1,64.3923  \n",
      "'epoch 1,64.4989  \n",
      "'epoch 1,64.6055  \n",
      "'epoch 1,64.7122  \n",
      "'epoch 1,64.8188  \n",
      "'epoch 1,64.9254  \n",
      "'epoch 1,65.0320  \n",
      "'epoch 1,65.1386  \n",
      "'epoch 1,65.2452  \n",
      "'epoch 1,65.3518  \n",
      "'epoch 1,65.4584  \n",
      "'epoch 1,65.5650  \n",
      "'epoch 1,65.6716  \n",
      "'epoch 1,65.7783  \n",
      "'epoch 1,65.8849  \n",
      "'epoch 1,65.9915  \n",
      "'epoch 1,66.0981  \n",
      "'epoch 1,66.2047  \n",
      "'epoch 1,66.3113  \n",
      "'epoch 1,66.4179  \n",
      "'epoch 1,66.5245  \n",
      "'epoch 1,66.6311  \n",
      "'epoch 1,66.7377  \n",
      "'epoch 1,66.8443  \n",
      "'epoch 1,66.9510  \n",
      "'epoch 1,67.0576  \n",
      "'epoch 1,67.1642  \n",
      "'epoch 1,67.2708  \n",
      "'epoch 1,67.3774  \n",
      "'epoch 1,67.4840  \n",
      "'epoch 1,67.5906  \n",
      "'epoch 1,67.6972  \n",
      "'epoch 1,67.8038  \n",
      "'epoch 1,67.9104  \n",
      "'epoch 1,68.0171  \n",
      "'epoch 1,68.1237  \n",
      "'epoch 1,68.2303  \n",
      "'epoch 1,68.3369  \n",
      "'epoch 1,68.4435  \n",
      "'epoch 1,68.5501  \n",
      "'epoch 1,68.6567  \n",
      "'epoch 1,68.7633  \n",
      "'epoch 1,68.8699  \n",
      "'epoch 1,68.9765  \n",
      "'epoch 1,69.0832  \n",
      "'epoch 1,69.1898  \n",
      "'epoch 1,69.2964  \n",
      "'epoch 1,69.4030  \n",
      "'epoch 1,69.5096  \n",
      "'epoch 1,69.6162  \n",
      "'epoch 1,69.7228  \n",
      "'epoch 1,69.8294  \n",
      "'epoch 1,69.9360  \n",
      "'epoch 1,70.0426  \n",
      "'epoch 1,70.1493  \n",
      "'epoch 1,70.2559  \n",
      "'epoch 1,70.3625  \n",
      "'epoch 1,70.4691  \n",
      "'epoch 1,70.5757  \n",
      "'epoch 1,70.6823  \n",
      "'epoch 1,70.7889  \n",
      "'epoch 1,70.8955  \n",
      "'epoch 1,71.0021  \n",
      "'epoch 1,71.1087  \n",
      "'epoch 1,71.2154  \n",
      "'epoch 1,71.3220  \n",
      "'epoch 1,71.4286  \n",
      "'epoch 1,71.5352  \n",
      "'epoch 1,71.6418  \n",
      "'epoch 1,71.7484  \n",
      "'epoch 1,71.8550  \n",
      "'epoch 1,71.9616  \n",
      "'epoch 1,72.0682  \n",
      "'epoch 1,72.1748  \n",
      "'epoch 1,72.2814  \n",
      "'epoch 1,72.3881  \n",
      "'epoch 1,72.4947  \n",
      "'epoch 1,72.6013  \n",
      "'epoch 1,72.7079  \n",
      "'epoch 1,72.8145  \n",
      "'epoch 1,72.9211  \n",
      "'epoch 1,73.0277  \n",
      "'epoch 1,73.1343  \n",
      "'epoch 1,73.2409  \n",
      "'epoch 1,73.3475  \n",
      "'epoch 1,73.4542  \n",
      "'epoch 1,73.5608  \n",
      "'epoch 1,73.6674  \n",
      "'epoch 1,73.7740  \n",
      "'epoch 1,73.8806  \n",
      "'epoch 1,73.9872  \n",
      "'epoch 1,74.0938  \n",
      "'epoch 1,74.2004  \n",
      "'epoch 1,74.3070  \n",
      "'epoch 1,74.4136  \n",
      "'epoch 1,74.5203  \n",
      "'epoch 1,74.6269  \n",
      "'epoch 1,74.7335  \n",
      "'epoch 1,74.8401  \n",
      "'epoch 1,74.9467  \n",
      "'epoch 1,75.0533  \n",
      "'epoch 1,75.1599  \n",
      "'epoch 1,75.2665  \n",
      "'epoch 1,75.3731  \n",
      "'epoch 1,75.4797  \n",
      "'epoch 1,75.5864  \n",
      "'epoch 1,75.6930  \n",
      "'epoch 1,75.7996  \n",
      "'epoch 1,75.9062  \n",
      "'epoch 1,76.0128  \n",
      "'epoch 1,76.1194  \n",
      "'epoch 1,76.2260  \n",
      "'epoch 1,76.3326  \n",
      "'epoch 1,76.4392  \n",
      "'epoch 1,76.5458  \n",
      "'epoch 1,76.6525  \n",
      "'epoch 1,76.7591  \n",
      "'epoch 1,76.8657  \n",
      "'epoch 1,76.9723  \n",
      "'epoch 1,77.0789  \n",
      "'epoch 1,77.1855  \n",
      "'epoch 1,77.2921  \n",
      "'epoch 1,77.3987  \n",
      "'epoch 1,77.5053  \n",
      "'epoch 1,77.6119  \n",
      "'epoch 1,77.7186  \n",
      "'epoch 1,77.8252  \n",
      "'epoch 1,77.9318  \n",
      "'epoch 1,78.0384  \n",
      "'epoch 1,78.1450  \n",
      "'epoch 1,78.2516  \n",
      "'epoch 1,78.3582  \n",
      "'epoch 1,78.4648  \n",
      "'epoch 1,78.5714  \n",
      "'epoch 1,78.6780  \n",
      "'epoch 1,78.7846  \n",
      "'epoch 1,78.8913  \n",
      "'epoch 1,78.9979  \n",
      "'epoch 1,79.1045  \n",
      "'epoch 1,79.2111  \n",
      "'epoch 1,79.3177  \n",
      "'epoch 1,79.4243  \n",
      "'epoch 1,79.5309  \n",
      "'epoch 1,79.6375  \n",
      "'epoch 1,79.7441  \n",
      "'epoch 1,79.8507  \n",
      "'epoch 1,79.9574  \n",
      "'epoch 1,80.0640  \n",
      "'epoch 1,80.1706  \n",
      "'epoch 1,80.2772  \n",
      "'epoch 1,80.3838  \n",
      "'epoch 1,80.4904  \n",
      "'epoch 1,80.5970  \n",
      "'epoch 1,80.7036  \n",
      "'epoch 1,80.8102  \n",
      "'epoch 1,80.9168  \n",
      "'epoch 1,81.0235  \n",
      "'epoch 1,81.1301  \n",
      "'epoch 1,81.2367  \n",
      "'epoch 1,81.3433  \n",
      "'epoch 1,81.4499  \n",
      "'epoch 1,81.5565  \n",
      "'epoch 1,81.6631  \n",
      "'epoch 1,81.7697  \n",
      "'epoch 1,81.8763  \n",
      "'epoch 1,81.9829  \n",
      "'epoch 1,82.0896  \n",
      "'epoch 1,82.1962  \n",
      "'epoch 1,82.3028  \n",
      "'epoch 1,82.4094  \n",
      "'epoch 1,82.5160  \n",
      "'epoch 1,82.6226  \n",
      "'epoch 1,82.7292  \n",
      "'epoch 1,82.8358  \n",
      "'epoch 1,82.9424  \n",
      "'epoch 1,83.0490  \n",
      "'epoch 1,83.1557  \n",
      "'epoch 1,83.2623  \n",
      "'epoch 1,83.3689  \n",
      "'epoch 1,83.4755  \n",
      "'epoch 1,83.5821  \n",
      "'epoch 1,83.6887  \n",
      "'epoch 1,83.7953  \n",
      "'epoch 1,83.9019  \n",
      "'epoch 1,84.0085  \n",
      "'epoch 1,84.1151  \n",
      "'epoch 1,84.2217  \n",
      "'epoch 1,84.3284  \n",
      "'epoch 1,84.4350  \n",
      "'epoch 1,84.5416  \n",
      "'epoch 1,84.6482  \n",
      "'epoch 1,84.7548  \n",
      "'epoch 1,84.8614  \n",
      "'epoch 1,84.9680  \n",
      "'epoch 1,85.0746  \n",
      "'epoch 1,85.1812  \n",
      "'epoch 1,85.2878  \n",
      "'epoch 1,85.3945  \n",
      "'epoch 1,85.5011  \n",
      "'epoch 1,85.6077  \n",
      "'epoch 1,85.7143  \n",
      "'epoch 1,85.8209  \n",
      "'epoch 1,85.9275  \n",
      "'epoch 1,86.0341  \n",
      "'epoch 1,86.1407  \n",
      "'epoch 1,86.2473  \n",
      "'epoch 1,86.3539  \n",
      "'epoch 1,86.4606  \n",
      "'epoch 1,86.5672  \n",
      "'epoch 1,86.6738  \n",
      "'epoch 1,86.7804  \n",
      "'epoch 1,86.8870  \n",
      "'epoch 1,86.9936  \n",
      "'epoch 1,87.1002  \n",
      "'epoch 1,87.2068  \n",
      "'epoch 1,87.3134  \n",
      "'epoch 1,87.4200  \n",
      "'epoch 1,87.5267  \n",
      "'epoch 1,87.6333  \n",
      "'epoch 1,87.7399  \n",
      "'epoch 1,87.8465  \n",
      "'epoch 1,87.9531  \n",
      "'epoch 1,88.0597  \n",
      "'epoch 1,88.1663  \n",
      "'epoch 1,88.2729  \n",
      "'epoch 1,88.3795  \n",
      "'epoch 1,88.4861  \n",
      "'epoch 1,88.5928  \n",
      "'epoch 1,88.6994  \n",
      "'epoch 1,88.8060  \n",
      "'epoch 1,88.9126  \n",
      "'epoch 1,89.0192  \n",
      "'epoch 1,89.1258  \n",
      "'epoch 1,89.2324  \n",
      "'epoch 1,89.3390  \n",
      "'epoch 1,89.4456  \n",
      "'epoch 1,89.5522  \n",
      "'epoch 1,89.6588  \n",
      "'epoch 1,89.7655  \n",
      "'epoch 1,89.8721  \n",
      "'epoch 1,89.9787  \n",
      "'epoch 1,90.0853  \n",
      "'epoch 1,90.1919  \n",
      "'epoch 1,90.2985  \n",
      "'epoch 1,90.4051  \n",
      "'epoch 1,90.5117  \n",
      "'epoch 1,90.6183  \n",
      "'epoch 1,90.7249  \n",
      "'epoch 1,90.8316  \n",
      "'epoch 1,90.9382  \n",
      "'epoch 1,91.0448  \n",
      "'epoch 1,91.1514  \n",
      "'epoch 1,91.2580  \n",
      "'epoch 1,91.3646  \n",
      "'epoch 1,91.4712  \n",
      "'epoch 1,91.5778  \n",
      "'epoch 1,91.6844  \n",
      "'epoch 1,91.7910  \n",
      "'epoch 1,91.8977  \n",
      "'epoch 1,92.0043  \n",
      "'epoch 1,92.1109  \n",
      "'epoch 1,92.2175  \n",
      "'epoch 1,92.3241  \n",
      "'epoch 1,92.4307  \n",
      "'epoch 1,92.5373  \n",
      "'epoch 1,92.6439  \n",
      "'epoch 1,92.7505  \n",
      "'epoch 1,92.8571  \n",
      "'epoch 1,92.9638  \n",
      "'epoch 1,93.0704  \n",
      "'epoch 1,93.1770  \n",
      "'epoch 1,93.2836  \n",
      "'epoch 1,93.3902  \n",
      "'epoch 1,93.4968  \n",
      "'epoch 1,93.6034  \n",
      "'epoch 1,93.7100  \n",
      "'epoch 1,93.8166  \n",
      "'epoch 1,93.9232  \n",
      "'epoch 1,94.0299  \n",
      "'epoch 1,94.1365  \n",
      "'epoch 1,94.2431  \n",
      "'epoch 1,94.3497  \n",
      "'epoch 1,94.4563  \n",
      "'epoch 1,94.5629  \n",
      "'epoch 1,94.6695  \n",
      "'epoch 1,94.7761  \n",
      "'epoch 1,94.8827  \n",
      "'epoch 1,94.9893  \n",
      "'epoch 1,95.0959  \n",
      "'epoch 1,95.2026  \n",
      "'epoch 1,95.3092  \n",
      "'epoch 1,95.4158  \n",
      "'epoch 1,95.5224  \n",
      "'epoch 1,95.6290  \n",
      "'epoch 1,95.7356  \n",
      "'epoch 1,95.8422  \n",
      "'epoch 1,95.9488  \n",
      "'epoch 1,96.0554  \n",
      "'epoch 1,96.1620  \n",
      "'epoch 1,96.2687  \n",
      "'epoch 1,96.3753  \n",
      "'epoch 1,96.4819  \n",
      "'epoch 1,96.5885  \n",
      "'epoch 1,96.6951  \n",
      "'epoch 1,96.8017  \n",
      "'epoch 1,96.9083  \n",
      "'epoch 1,97.0149  \n",
      "'epoch 1,97.1215  \n",
      "'epoch 1,97.2281  \n",
      "'epoch 1,97.3348  \n",
      "'epoch 1,97.4414  \n",
      "'epoch 1,97.5480  \n",
      "'epoch 1,97.6546  \n",
      "'epoch 1,97.7612  \n",
      "'epoch 1,97.8678  \n",
      "'epoch 1,97.9744  \n",
      "'epoch 1,98.0810  \n",
      "'epoch 1,98.1876  \n",
      "'epoch 1,98.2942  \n",
      "'epoch 1,98.4009  \n",
      "'epoch 1,98.5075  \n",
      "'epoch 1,98.6141  \n",
      "'epoch 1,98.7207  \n",
      "'epoch 1,98.8273  \n",
      "'epoch 1,98.9339  \n",
      "'epoch 1,99.0405  \n",
      "'epoch 1,99.1471  \n",
      "'epoch 1,99.2537  \n",
      "'epoch 1,99.3603  \n",
      "'epoch 1,99.4670  \n",
      "'epoch 1,99.5736  \n",
      "'epoch 1,99.6802  \n",
      "'epoch 1,99.7868  \n",
      "'epoch 1,99.8934  \n",
      "'epoch 1,100.0000  \n",
      "epoch 1, loss 0.0393, train acc 0.107, test acc 0.111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = net.to(device)\n",
    "loss.to(device)\n",
    "def train_ch(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, optimizer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        num_train_iter = len(train_iter)\n",
    "        t = 1 \n",
    "        for X, y in train_iter:\n",
    "            print(\"'epoch %d,%.4f  \"% (epoch + 1,t / num_train_iter * 100))\n",
    "            t+=1 \n",
    "            X = torch.cat((X, X, X), 1).to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).sum()\n",
    "            # print(1)\n",
    "            # 梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            # print(2)\n",
    "            l.backward()\n",
    "            # print(3)\n",
    "            optimizer.step() \n",
    "            # print(4)\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "\n",
    "train_ch(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "93bd4c27-f5af-413c-a20c-2b994e54595d"
   },
   "source": [
    "# 练习题\n",
    "选择题：\n",
    "1. 假设我们将源模型的输出层改成输出大小为目标数据集类别个数的输出层，则对于这个新的输出层如何初始化  \n",
    "    a. 复制源模型的参数进行初始化  \n",
    "    b. 随机初始化参数  \n",
    "    c. 用全零初始化参数  \n",
    "    d. 不需要初始化  \n",
    "2. 假设我们将源模型的输出层改成输出大小为目标数据集类别个数的输出层，在训练过程中下列说法正确的是  \n",
    "    a. 对输出层使用较大的学习率，对其他层使用较小的学习率。  \n",
    "    b. 对输出层使用较小的学习率，对其他层使用较大的学习率。  \n",
    "    c. 对输出层和其他层使用相同大小的学习率。    \n",
    "    d. 对输出层进行微调，其他层保持参数不变，不需要学习。  \n",
    "\n",
    "答案：\n",
    "1. b\n",
    "2. a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "631b1fba-e666-4832-8bfd-0254acf57111"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [],
   "description": "",
   "notebookId": "163488",
   "source": "dsw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
